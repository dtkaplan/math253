<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Notes for Statistical Computing &amp; Machine Learning</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Notes and other materials for Math 253 at Macalester College.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Notes for Statistical Computing &amp; Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and other materials for Math 253 at Macalester College." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Notes for Statistical Computing &amp; Machine Learning" />
  
  <meta name="twitter:description" content="Notes and other materials for Math 253 at Macalester College." />
  

<meta name="author" content="Daniel Kaplan">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="cross-validation-and-bootstrapping.html">
<link rel="next" href="nonlinearity-in-linear-models.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math 253 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#math-253-and-the-macalester-statistics-curriculum"><i class="fa fa-check"></i>Math 253 and the Macalester statistics curriculum</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#these-notes-are-written-in-bookdown"><i class="fa fa-check"></i>These notes are written in Bookdown</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#statistical-and-machine-learning"><i class="fa fa-check"></i><b>1.1</b> Statistical and Machine Learning</a><ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#example-1-machine-translation-of-natural-languages"><i class="fa fa-check"></i><b>1.1.1</b> Example 1: Machine translation of natural languages</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#example-2-from-library-catalogs-to-latent-semantic-indexing"><i class="fa fa-check"></i><b>1.1.2</b> Example 2: From library catalogs to latent semantic indexing</a></li>
<li class="chapter" data-level="1.1.3" data-path="introduction.html"><a href="introduction.html#computing-technique"><i class="fa fa-check"></i><b>1.1.3</b> Computing technique</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#review-of-day-1"><i class="fa fa-check"></i><b>1.2</b> Review of Day 1</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#theoretical-concepts-isl-2.1"><i class="fa fa-check"></i><b>1.3</b> Theoretical concepts ISL ยง2.1</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#statistics-concepts"><i class="fa fa-check"></i><b>1.3.1</b> Statistics concepts</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#computing-concepts"><i class="fa fa-check"></i><b>1.3.2</b> Computing concepts</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#cross-fertilization"><i class="fa fa-check"></i><b>1.3.3</b> Cross fertilization</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#many-techniques"><i class="fa fa-check"></i><b>1.4</b> Many techniques</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.4.1</b> Unsupervised learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction.html"><a href="introduction.html#supervised-learning"><i class="fa fa-check"></i><b>1.4.2</b> Supervised learning:</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#basic-dicotomies-in-machine-learning"><i class="fa fa-check"></i><b>1.5</b> Basic dicotomies in machine learning</a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#purposes-for-learning"><i class="fa fa-check"></i><b>1.5.1</b> Purposes for learning:</a></li>
<li class="chapter" data-level="1.5.2" data-path="introduction.html"><a href="introduction.html#dicotomies"><i class="fa fa-check"></i><b>1.5.2</b> Dicotomies</a></li>
<li class="chapter" data-level="1.5.3" data-path="introduction.html"><a href="introduction.html#prediction-versus-mechanism"><i class="fa fa-check"></i><b>1.5.3</b> Prediction versus mechanism</a></li>
<li class="chapter" data-level="1.5.4" data-path="introduction.html"><a href="introduction.html#flexibility-versus-variance"><i class="fa fa-check"></i><b>1.5.4</b> Flexibility versus variance</a></li>
<li class="chapter" data-level="1.5.5" data-path="introduction.html"><a href="introduction.html#black-box-vs-interpretable-models"><i class="fa fa-check"></i><b>1.5.5</b> Black box vs interpretable models</a></li>
<li class="chapter" data-level="1.5.6" data-path="introduction.html"><a href="introduction.html#reducible-versus-irreducible-error"><i class="fa fa-check"></i><b>1.5.6</b> Reducible versus irreducible error</a></li>
<li class="chapter" data-level="1.5.7" data-path="introduction.html"><a href="introduction.html#regression-versus-classification"><i class="fa fa-check"></i><b>1.5.7</b> Regression versus classification</a></li>
<li class="chapter" data-level="1.5.8" data-path="introduction.html"><a href="introduction.html#supervised-versus-unsupervised"><i class="fa fa-check"></i><b>1.5.8</b> Supervised versus unsupervised</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#programming-activity-1"><i class="fa fa-check"></i><b>1.6</b> Programming Activity 1</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#review-of-day-2"><i class="fa fa-check"></i><b>1.7</b> Review of Day 2</a><ul>
<li class="chapter" data-level="1.7.1" data-path="introduction.html"><a href="introduction.html#trade-offsdicotomies"><i class="fa fa-check"></i><b>1.7.1</b> Trade-offs/Dicotomies</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#a-classifier-example"><i class="fa fa-check"></i><b>1.8</b> A Classifier example</a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#programming-activity-2"><i class="fa fa-check"></i><b>1.9</b> Programming Activity 2</a></li>
<li class="chapter" data-level="1.10" data-path="introduction.html"><a href="introduction.html#day-3-theory-accuracy-precision-and-bias"><i class="fa fa-check"></i><b>1.10</b> Day 3 theory: accuracy, precision, and bias</a><ul>
<li class="chapter" data-level="1.10.1" data-path="introduction.html"><a href="introduction.html#figure-2.10"><i class="fa fa-check"></i><b>1.10.1</b> Figure 2.10</a></li>
<li class="chapter" data-level="1.10.2" data-path="introduction.html"><a href="introduction.html#another-example-a-smoother-simulated-fx."><i class="fa fa-check"></i><b>1.10.2</b> Another example: A smoother simulated <span class="math inline">\(f(x)\)</span>.</a></li>
<li class="chapter" data-level="1.10.3" data-path="introduction.html"><a href="introduction.html#whats-the-best-of-these-models"><i class="fa fa-check"></i><b>1.10.3</b> Whatโs the โbestโ of these models?</a></li>
<li class="chapter" data-level="1.10.4" data-path="introduction.html"><a href="introduction.html#why-is-testing-mse-u-shaped"><i class="fa fa-check"></i><b>1.10.4</b> Why is testing MSE U-shaped?</a></li>
<li class="chapter" data-level="1.10.5" data-path="introduction.html"><a href="introduction.html#measuring-the-variance-of-independent-sources-of-variation"><i class="fa fa-check"></i><b>1.10.5</b> Measuring the variance of independent sources of variation</a></li>
<li class="chapter" data-level="1.10.6" data-path="introduction.html"><a href="introduction.html#equation-2.7"><i class="fa fa-check"></i><b>1.10.6</b> Equation 2.7</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="introduction.html"><a href="introduction.html#programming-activity-3"><i class="fa fa-check"></i><b>1.11</b> Programming Activity 3</a></li>
<li class="chapter" data-level="1.12" data-path="introduction.html"><a href="introduction.html#review-of-day-3"><i class="fa fa-check"></i><b>1.12</b> Review of Day 3</a></li>
<li class="chapter" data-level="1.13" data-path="introduction.html"><a href="introduction.html#start-thursday-15-sept."><i class="fa fa-check"></i><b>1.13</b> Start Thursday 15 Sept.</a></li>
</ul></li>
<li class="part"><span><b>I Topic I: Linear Regression</b></span><ul>
<li class="chapter" data-level="1.14" data-path="introduction.html"><a href="introduction.html#day-4-preview"><i class="fa fa-check"></i><b>1.14</b> Day 4 Preview</a></li>
<li class="chapter" data-level="1.15" data-path="introduction.html"><a href="introduction.html#small-data"><i class="fa fa-check"></i><b>1.15</b> Small data</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>2</b> Notes</a><ul>
<li class="chapter" data-level="2.1" data-path="notes.html"><a href="notes.html#review-of-day-4-sept-15-2017"><i class="fa fa-check"></i><b>2.1</b> Review of Day 4, Sept 15, 2017</a></li>
<li class="chapter" data-level="2.2" data-path="notes.html"><a href="notes.html#regression-and-interpretability"><i class="fa fa-check"></i><b>2.2</b> Regression and Interpretability</a></li>
<li class="chapter" data-level="2.3" data-path="notes.html"><a href="notes.html#toward-an-automated-regression-process"><i class="fa fa-check"></i><b>2.3</b> Toward an automated regression process</a></li>
<li class="chapter" data-level="2.4" data-path="notes.html"><a href="notes.html#selecting-model-terms"><i class="fa fa-check"></i><b>2.4</b> Selecting model terms</a></li>
<li class="chapter" data-level="2.5" data-path="notes.html"><a href="notes.html#programming-basics-graphics"><i class="fa fa-check"></i><b>2.5</b> Programming basics: Graphics</a></li>
<li class="chapter" data-level="2.6" data-path="notes.html"><a href="notes.html#in-class-programming-activity"><i class="fa fa-check"></i><b>2.6</b> In-class programming activity</a></li>
<li class="chapter" data-level="2.7" data-path="notes.html"><a href="notes.html#day-5-summary"><i class="fa fa-check"></i><b>2.7</b> Day 5 Summary</a><ul>
<li class="chapter" data-level="2.7.1" data-path="notes.html"><a href="notes.html#linear-regression"><i class="fa fa-check"></i><b>2.7.1</b> Linear regression</a></li>
<li class="chapter" data-level="2.7.2" data-path="notes.html"><a href="notes.html#coefficients-as-quantities"><i class="fa fa-check"></i><b>2.7.2</b> Coefficients as quantities</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="notes.html"><a href="notes.html#in-class-programming-activity-1"><i class="fa fa-check"></i><b>2.8</b> In-class programming activity</a></li>
<li class="chapter" data-level="2.9" data-path="notes.html"><a href="notes.html#day-6-summary"><i class="fa fa-check"></i><b>2.9</b> Day 6 Summary</a></li>
<li class="chapter" data-level="2.10" data-path="notes.html"><a href="notes.html#measuring-accuracy-of-the-model"><i class="fa fa-check"></i><b>2.10</b> Measuring Accuracy of the Model</a></li>
<li class="chapter" data-level="2.11" data-path="notes.html"><a href="notes.html#bias-of-the-model"><i class="fa fa-check"></i><b>2.11</b> Bias of the model</a><ul>
<li class="chapter" data-level="2.11.1" data-path="notes.html"><a href="notes.html#theory-of-whole-model-anova."><i class="fa fa-check"></i><b>2.11.1</b> Theory of whole-model ANOVA.</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="notes.html"><a href="notes.html#forward-backward-and-mixed-selection"><i class="fa fa-check"></i><b>2.12</b> Forward, backward and mixed selection</a></li>
<li class="chapter" data-level="2.13" data-path="notes.html"><a href="notes.html#programming-basics-functions"><i class="fa fa-check"></i><b>2.13</b> Programming Basics: Functions</a></li>
<li class="chapter" data-level="2.14" data-path="notes.html"><a href="notes.html#in-class-programming-activity-2"><i class="fa fa-check"></i><b>2.14</b> In-class programming activity</a></li>
<li class="chapter" data-level="2.15" data-path="notes.html"><a href="notes.html#review-of-day-7"><i class="fa fa-check"></i><b>2.15</b> Review of Day 7</a></li>
<li class="chapter" data-level="2.16" data-path="notes.html"><a href="notes.html#using-predict-to-calculate-precision"><i class="fa fa-check"></i><b>2.16</b> Using predict() to calculate precision</a></li>
<li class="chapter" data-level="2.17" data-path="notes.html"><a href="notes.html#conclusion"><i class="fa fa-check"></i><b>2.17</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html"><i class="fa fa-check"></i><b>3</b> Foundations: linear algebra, likelihood and Bayesโ rule</a><ul>
<li class="chapter" data-level="3.1" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#linear-algebra"><i class="fa fa-check"></i><b>3.1</b> Linear Algebra</a></li>
<li class="chapter" data-level="3.2" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#arithmetic-of-linear-algebra-operations"><i class="fa fa-check"></i><b>3.2</b> Arithmetic of linear algebra operations</a></li>
<li class="chapter" data-level="3.3" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#the-geometry-of-fitting"><i class="fa fa-check"></i><b>3.3</b> The geometry of fitting</a></li>
<li class="chapter" data-level="3.4" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#precision-of-the-coefficients"><i class="fa fa-check"></i><b>3.4</b> Precision of the coefficients</a></li>
<li class="chapter" data-level="3.5" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#likelihood-and-bayes"><i class="fa fa-check"></i><b>3.5</b> Likelihood and Bayes</a></li>
<li class="chapter" data-level="3.6" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#summary-of-day-8"><i class="fa fa-check"></i><b>3.6</b> Summary of Day 8</a></li>
<li class="chapter" data-level="3.7" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#day-9-announcements"><i class="fa fa-check"></i><b>3.7</b> Day 9 Announcements</a><ul>
<li class="chapter" data-level="3.7.1" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#whats-a-probability"><i class="fa fa-check"></i><b>3.7.1</b> Whatโs a probability?</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#conditional-probability"><i class="fa fa-check"></i><b>3.8</b> Conditional probability</a></li>
<li class="chapter" data-level="3.9" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#inverting-conditional-probabilities"><i class="fa fa-check"></i><b>3.9</b> Inverting conditional probabilities</a></li>
<li class="chapter" data-level="3.10" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#summary-of-day-9"><i class="fa fa-check"></i><b>3.10</b> Summary of Day 9</a></li>
<li class="chapter" data-level="3.11" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#likelihood-example"><i class="fa fa-check"></i><b>3.11</b> Likelihood example</a></li>
<li class="chapter" data-level="3.12" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#exponential-probability-density"><i class="fa fa-check"></i><b>3.12</b> Exponential probability density</a><ul>
<li class="chapter" data-level="3.12.1" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#meanwhile-further-north"><i class="fa fa-check"></i><b>3.12.1</b> Meanwhile, further north โฆ</a></li>
</ul></li>
<li class="chapter" data-level="3.13" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#california-earthquake-warning-reprise"><i class="fa fa-check"></i><b>3.13</b> California earthquake warning, reprise</a></li>
<li class="chapter" data-level="3.14" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#the-price-is-right"><i class="fa fa-check"></i><b>3.14</b> The Price is Right!</a></li>
<li class="chapter" data-level="3.15" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#from-likelihood-to-bayes"><i class="fa fa-check"></i><b>3.15</b> From likelihood to Bayes</a></li>
<li class="chapter" data-level="3.16" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#choosing-models-using-maximum-likelihood"><i class="fa fa-check"></i><b>3.16</b> Choosing models using maximum likelihood</a></li>
<li class="chapter" data-level="3.17" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#day-9-review"><i class="fa fa-check"></i><b>3.17</b> Day 9 Review</a></li>
<li class="chapter" data-level="3.18" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#reading-what-is-bayesian-statistics"><i class="fa fa-check"></i><b>3.18</b> Reading: <em>What is Bayesian Statistics</em></a></li>
<li class="chapter" data-level="3.19" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#programming-basics-conditionals"><i class="fa fa-check"></i><b>3.19</b> Programming Basics: Conditionals</a></li>
<li class="chapter" data-level="3.20" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#ifelse-examples"><i class="fa fa-check"></i><b>3.20</b> <code>ifelse()</code> examples</a></li>
<li class="chapter" data-level="3.21" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#if-else-examples"><i class="fa fa-check"></i><b>3.21</b> if โฆ else โฆ examples</a></li>
<li class="chapter" data-level="3.22" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#simple"><i class="fa fa-check"></i><b>3.22</b> Simple</a></li>
<li class="chapter" data-level="3.23" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#blood-testing"><i class="fa fa-check"></i><b>3.23</b> Blood testing</a></li>
<li class="chapter" data-level="3.24" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#the-hyper-volume-of-the-hypersphere."><i class="fa fa-check"></i><b>3.24</b> The (hyper)-volume of the hypersphere.</a></li>
<li class="chapter" data-level="3.25" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#find-the-surface-area-d_n-rn-1."><i class="fa fa-check"></i><b>3.25</b> Find the surface area, <span class="math inline">\(D_n r^{n-1}\)</span>.</a></li>
<li class="chapter" data-level="3.26" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#in-class-programming-activity-3"><i class="fa fa-check"></i><b>3.26</b> In-class programming activity</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classifiers.html"><a href="classifiers.html"><i class="fa fa-check"></i><b>4</b> Classifiers</a><ul>
<li class="chapter" data-level="4.1" data-path="classifiers.html"><a href="classifiers.html#classification-overview"><i class="fa fa-check"></i><b>4.1</b> Classification overview</a></li>
<li class="chapter" data-level="4.2" data-path="classifiers.html"><a href="classifiers.html#day-10-preview"><i class="fa fa-check"></i><b>4.2</b> Day 10 preview</a></li>
<li class="chapter" data-level="4.3" data-path="classifiers.html"><a href="classifiers.html#probability-and-odds"><i class="fa fa-check"></i><b>4.3</b> Probability and odds</a></li>
<li class="chapter" data-level="4.4" data-path="classifiers.html"><a href="classifiers.html#log-odds"><i class="fa fa-check"></i><b>4.4</b> Log Odds</a></li>
<li class="chapter" data-level="4.5" data-path="classifiers.html"><a href="classifiers.html#why-use-odds"><i class="fa fa-check"></i><b>4.5</b> Why use odds?</a></li>
<li class="chapter" data-level="4.6" data-path="classifiers.html"><a href="classifiers.html#use-of-glm"><i class="fa fa-check"></i><b>4.6</b> Use of glm()</a></li>
<li class="chapter" data-level="4.7" data-path="classifiers.html"><a href="classifiers.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>4.7</b> Interpretation of coefficients</a></li>
<li class="chapter" data-level="4.8" data-path="classifiers.html"><a href="classifiers.html#example-logistic-regression-of-default"><i class="fa fa-check"></i><b>4.8</b> Example: Logistic regression of default</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html"><i class="fa fa-check"></i><b>5</b> Linear and Quadratic Discriminant Analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#example-default-on-student-loans"><i class="fa fa-check"></i><b>5.1</b> Example: Default on student loans</a></li>
<li class="chapter" data-level="5.2" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#a-bayes-rule-approach"><i class="fa fa-check"></i><b>5.2</b> A Bayesโ Rule approach</a></li>
<li class="chapter" data-level="5.3" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#univariate-gaussian"><i class="fa fa-check"></i><b>5.3</b> Univariate Gaussian</a></li>
<li class="chapter" data-level="5.4" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#uncorrelated-bivariate-gaussian"><i class="fa fa-check"></i><b>5.4</b> Uncorrelated bivariate gaussian</a></li>
<li class="chapter" data-level="5.5" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#bivariate-normal-distribution-with-correlations"><i class="fa fa-check"></i><b>5.5</b> Bivariate normal distribution with correlations</a></li>
<li class="chapter" data-level="5.6" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#shape-of-multivariate-gaussian"><i class="fa fa-check"></i><b>5.6</b> Shape of multivariate gaussian</a></li>
<li class="chapter" data-level="5.7" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#generating-bivariate-normal-from-independent"><i class="fa fa-check"></i><b>5.7</b> Generating bivariate normal from independent</a></li>
<li class="chapter" data-level="5.8" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#independent-variables-x_i"><i class="fa fa-check"></i><b>5.8</b> Independent variables <span class="math inline">\(x_i\)</span></a></li>
<li class="chapter" data-level="5.9" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#re-explaining-boldsymbolsigma"><i class="fa fa-check"></i><b>5.9</b> Re-explaining <span class="math inline">\(\boldsymbol\Sigma\)</span></a></li>
<li class="chapter" data-level="5.10" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#lda"><i class="fa fa-check"></i><b>5.10</b> LDA</a></li>
<li class="chapter" data-level="5.11" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#qda"><i class="fa fa-check"></i><b>5.11</b> QDA</a></li>
<li class="chapter" data-level="5.12" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#error-test-rates-on-various-classifiers"><i class="fa fa-check"></i><b>5.12</b> Error test rates on various classifiers</a></li>
<li class="chapter" data-level="5.13" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#error-rates"><i class="fa fa-check"></i><b>5.13</b> Error rates</a></li>
<li class="chapter" data-level="5.14" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#receiver-operating-curves"><i class="fa fa-check"></i><b>5.14</b> Receiver operating curves</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html"><i class="fa fa-check"></i><b>6</b> Cross-Validation and Bootstrapping</a><ul>
<li class="chapter" data-level="6.1" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#philosophical-approaches"><i class="fa fa-check"></i><b>6.1</b> Philosophical approaches</a><ul>
<li class="chapter" data-level="6.1.1" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#occams-razor-a-heuristic"><i class="fa fa-check"></i><b>6.1.1</b> Occamโs Razor: A heuristic</a></li>
<li class="chapter" data-level="6.1.2" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#einsteins-proverb"><i class="fa fa-check"></i><b>6.1.2</b> Einsteinโs proverb</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#operationalizing-model-choice"><i class="fa fa-check"></i><b>6.2</b> Operationalizing model choice</a></li>
<li class="chapter" data-level="6.3" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#some-definitions-of-better"><i class="fa fa-check"></i><b>6.3</b> Some definitions of โbetterโ</a></li>
<li class="chapter" data-level="6.4" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#training-and-testing"><i class="fa fa-check"></i><b>6.4</b> Training and Testing</a></li>
<li class="chapter" data-level="6.5" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#trade-off"><i class="fa fa-check"></i><b>6.5</b> Trade-off</a></li>
<li class="chapter" data-level="6.6" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#classical-theory-interlude"><i class="fa fa-check"></i><b>6.6</b> Classical theory interlude</a></li>
<li class="chapter" data-level="6.7" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#bootstrapping"><i class="fa fa-check"></i><b>6.7</b> Bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html"><i class="fa fa-check"></i><b>7</b> Regularization, shrinkage and dimension reduction</a><ul>
<li class="chapter" data-level="7.1" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#best-subset-selection"><i class="fa fa-check"></i><b>7.1</b> Best subset selection</a></li>
<li class="chapter" data-level="7.2" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#approximation-to-best-subset-selection"><i class="fa fa-check"></i><b>7.2</b> Approximation to best subset selection</a></li>
<li class="chapter" data-level="7.3" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#classical-theory-of-best-model-choice"><i class="fa fa-check"></i><b>7.3</b> Classical theory of best model choice</a></li>
<li class="chapter" data-level="7.4" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#optimization"><i class="fa fa-check"></i><b>7.4</b> Optimization</a><ul>
<li class="chapter" data-level="7.4.1" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#what-are-we-optimizing-over"><i class="fa fa-check"></i><b>7.4.1</b> What are we optimizing over?</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#shrinkage-methods"><i class="fa fa-check"></i><b>7.5</b> Shrinkage methods</a><ul>
<li class="chapter" data-level="7.5.1" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#ridge-regression"><i class="fa fa-check"></i><b>7.5.1</b> Ridge regression</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#lasso"><i class="fa fa-check"></i><b>7.6</b> LASSO</a></li>
<li class="chapter" data-level="7.7" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#review"><i class="fa fa-check"></i><b>7.7</b> Review</a></li>
<li class="chapter" data-level="7.8" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#multi-collinearity"><i class="fa fa-check"></i><b>7.8</b> Multi-collinearity</a></li>
<li class="chapter" data-level="7.9" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#creating-correlations"><i class="fa fa-check"></i><b>7.9</b> Creating correlations</a></li>
<li class="chapter" data-level="7.10" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#rank-1-matrices"><i class="fa fa-check"></i><b>7.10</b> Rank 1 Matrices</a></li>
<li class="chapter" data-level="7.11" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#idea-of-singular-values."><i class="fa fa-check"></i><b>7.11</b> Idea of singular values.</a></li>
<li class="chapter" data-level="7.12" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#dimension-reduction"><i class="fa fa-check"></i><b>7.12</b> Dimension reduction</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html"><i class="fa fa-check"></i><b>8</b> Nonlinearity in linear models</a><ul>
<li class="chapter" data-level="8.1" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#smoothers"><i class="fa fa-check"></i><b>8.1</b> Smoothers</a><ul>
<li class="chapter" data-level="8.1.1" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#ideas-of-smoothness"><i class="fa fa-check"></i><b>8.1.1</b> Ideas of smoothness</a></li>
<li class="chapter" data-level="8.1.2" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#polynomials"><i class="fa fa-check"></i><b>8.1.2</b> Polynomials</a></li>
<li class="chapter" data-level="8.1.3" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#the-model-matrix"><i class="fa fa-check"></i><b>8.1.3</b> The model matrix</a></li>
<li class="chapter" data-level="8.1.4" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#sigmoidal-functions"><i class="fa fa-check"></i><b>8.1.4</b> Sigmoidal Functions</a></li>
<li class="chapter" data-level="8.1.5" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#hat-functions"><i class="fa fa-check"></i><b>8.1.5</b> Hat functions</a></li>
<li class="chapter" data-level="8.1.6" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#fourier-analysis"><i class="fa fa-check"></i><b>8.1.6</b> Fourier analysis</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#steps"><i class="fa fa-check"></i><b>8.2</b> Steps</a></li>
<li class="chapter" data-level="8.3" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#other-functions"><i class="fa fa-check"></i><b>8.3</b> Other functions</a></li>
<li class="chapter" data-level="8.4" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#holes-in-the-data"><i class="fa fa-check"></i><b>8.4</b> Holes in the data</a></li>
<li class="chapter" data-level="8.5" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#bootstrapping-1"><i class="fa fa-check"></i><b>8.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="8.6" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#normal-theory-confidence-bands"><i class="fa fa-check"></i><b>8.6</b> Normal theory confidence bands</a></li>
<li class="chapter" data-level="8.7" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#splines"><i class="fa fa-check"></i><b>8.7</b> Splines</a><ul>
<li class="chapter" data-level="8.7.1" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#b-splines"><i class="fa fa-check"></i><b>8.7.1</b> B-splines</a></li>
<li class="chapter" data-level="8.7.2" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#natural-splines"><i class="fa fa-check"></i><b>8.7.2</b> Natural splines</a></li>
<li class="chapter" data-level="8.7.3" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#smoothing-splines"><i class="fa fa-check"></i><b>8.7.3</b> Smoothing splines</a></li>
<li class="chapter" data-level="8.7.4" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#smoothers-in-k-dimensions"><i class="fa fa-check"></i><b>8.7.4</b> Smoothers in k dimensions</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#gams"><i class="fa fa-check"></i><b>8.8</b> GAMS</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="programming-activity.html"><a href="programming-activity.html"><i class="fa fa-check"></i><b>9</b> Programming Activity</a></li>
<li class="chapter" data-level="10" data-path="where-to-place-knots.html"><a href="where-to-place-knots.html"><i class="fa fa-check"></i><b>10</b> Where to place knots?</a></li>
<li class="chapter" data-level="11" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html"><i class="fa fa-check"></i><b>11</b> Trees for Regression and Classification</a><ul>
<li class="chapter" data-level="11.1" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html#splitting-criteria-for-classification-trees"><i class="fa fa-check"></i><b>11.1</b> Splitting Criteria for Classification Trees</a></li>
<li class="chapter" data-level="11.2" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html#variable-importance"><i class="fa fa-check"></i><b>11.2</b> Variable importance</a></li>
<li class="chapter" data-level="11.3" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html#avoiding-overfitting"><i class="fa fa-check"></i><b>11.3</b> Avoiding overfitting</a></li>
<li class="chapter" data-level="11.4" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html#pruning"><i class="fa fa-check"></i><b>11.4</b> Pruning</a></li>
<li class="chapter" data-level="11.5" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html#averaging"><i class="fa fa-check"></i><b>11.5</b> Averaging</a></li>
<li class="chapter" data-level="11.6" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html#shrinking-boosting"><i class="fa fa-check"></i><b>11.6</b> Shrinking (โBoostingโ)</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html"><i class="fa fa-check"></i><b>12</b> Support Vector Classifiers</a><ul>
<li class="chapter" data-level="12.1" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#lines-planes-and-hyperplanes"><i class="fa fa-check"></i><b>12.1</b> Lines, planes, and hyperplanes</a><ul>
<li class="chapter" data-level="12.1.1" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#rescaling-x"><i class="fa fa-check"></i><b>12.1.1</b> Rescaling X</a></li>
<li class="chapter" data-level="12.1.2" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#impose-an-absolute-constraint"><i class="fa fa-check"></i><b>12.1.2</b> Impose an absolute constraint</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#optimizing-within-the-constraint"><i class="fa fa-check"></i><b>12.2</b> Optimizing within the constraint</a></li>
<li class="chapter" data-level="12.3" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#allowing-violations-of-the-boundary"><i class="fa fa-check"></i><b>12.3</b> Allowing violations of the boundary</a></li>
<li class="chapter" data-level="12.4" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#nonlinear-boundaries"><i class="fa fa-check"></i><b>12.4</b> Nonlinear Boundaries</a></li>
<li class="chapter" data-level="12.5" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#support-vector-machine"><i class="fa fa-check"></i><b>12.5</b> Support Vector Machine</a></li>
<li class="chapter" data-level="12.6" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#kernels"><i class="fa fa-check"></i><b>12.6</b> Kernels</a></li>
<li class="chapter" data-level="12.7" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#svm-versus-logistic-regression"><i class="fa fa-check"></i><b>12.7</b> SVM versus logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="programming-basics.html"><a href="programming-basics.html"><i class="fa fa-check"></i><b>13</b> Programming Basics</a><ul>
<li class="chapter" data-level="13.1" data-path="programming-basics.html"><a href="programming-basics.html#programming-basics-i-names-classes-and-objects-progbasics1"><i class="fa fa-check"></i><b>13.1</b> Programming Basics I: Names, classes, and objects {progbasics1}</a><ul>
<li class="chapter" data-level="13.1.1" data-path="programming-basics.html"><a href="programming-basics.html#names"><i class="fa fa-check"></i><b>13.1.1</b> Names</a></li>
<li class="chapter" data-level="13.1.2" data-path="programming-basics.html"><a href="programming-basics.html#objects"><i class="fa fa-check"></i><b>13.1.2</b> Objects</a></li>
<li class="chapter" data-level="13.1.3" data-path="programming-basics.html"><a href="programming-basics.html#vectors"><i class="fa fa-check"></i><b>13.1.3</b> Vectors</a></li>
<li class="chapter" data-level="13.1.4" data-path="programming-basics.html"><a href="programming-basics.html#matrices"><i class="fa fa-check"></i><b>13.1.4</b> Matrices</a></li>
<li class="chapter" data-level="13.1.5" data-path="programming-basics.html"><a href="programming-basics.html#lists"><i class="fa fa-check"></i><b>13.1.5</b> Lists</a></li>
<li class="chapter" data-level="13.1.6" data-path="programming-basics.html"><a href="programming-basics.html#functions"><i class="fa fa-check"></i><b>13.1.6</b> Functions</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="programming-basics.html"><a href="programming-basics.html#programming-basics-linear-models"><i class="fa fa-check"></i><b>13.2</b> Programming basics: Linear Models</a><ul>
<li class="chapter" data-level="13.2.1" data-path="programming-basics.html"><a href="programming-basics.html#graphics-basics"><i class="fa fa-check"></i><b>13.2.1</b> Graphics basics</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="programming-basics.html"><a href="programming-basics.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>13.3</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="13.4" data-path="programming-basics.html"><a href="programming-basics.html#loopsiteration"><i class="fa fa-check"></i><b>13.4</b> Loops/Iteration</a></li>
<li class="chapter" data-level="13.5" data-path="programming-basics.html"><a href="programming-basics.html#parts-of-a-loop"><i class="fa fa-check"></i><b>13.5</b> Parts of a loop</a></li>
<li class="chapter" data-level="13.6" data-path="programming-basics.html"><a href="programming-basics.html#trivial-examples"><i class="fa fa-check"></i><b>13.6</b> Trivial examples</a></li>
<li class="chapter" data-level="13.7" data-path="programming-basics.html"><a href="programming-basics.html#bootstrapping-2"><i class="fa fa-check"></i><b>13.7</b> Bootstrapping</a></li>
<li class="chapter" data-level="13.8" data-path="programming-basics.html"><a href="programming-basics.html#leave-one-out-cross-validation."><i class="fa fa-check"></i><b>13.8</b> Leave-one-out cross-validation.</a></li>
<li class="chapter" data-level="13.9" data-path="programming-basics.html"><a href="programming-basics.html#building-a-package"><i class="fa fa-check"></i><b>13.9</b> Building a package</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i>Appendices</a></li>
<li class="chapter" data-level="" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html"><i class="fa fa-check"></i>Connecting RStudio to your GitHub repository</a><ul>
<li class="chapter" data-level="13.10" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html#setting-up-rstudio"><i class="fa fa-check"></i><b>13.10</b> Setting up RStudio</a></li>
<li class="chapter" data-level="13.11" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html#setting-up-your-math-253-repository"><i class="fa fa-check"></i><b>13.11</b> Setting up your Math 253 repository</a></li>
<li class="chapter" data-level="13.12" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html#using-your-repository"><i class="fa fa-check"></i><b>13.12</b> Using your repository</a></li>
<li class="chapter" data-level="13.13" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html#why-are-we-doing-this"><i class="fa fa-check"></i><b>13.13</b> Why are we doing this?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="instructions-for-the-publishing-system-bookdown.html"><a href="instructions-for-the-publishing-system-bookdown.html"><i class="fa fa-check"></i>Instructions for the publishing system: Bookdown</a></li>
<li class="divider"></li>
<li><a href="https://github.com/dtkaplan/math253" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for Statistical Computing &amp; Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regularization-shrinkage-and-dimension-reduction" class="section level1">
<h1><span class="header-section-number">Topic 7</span> Regularization, shrinkage and dimension reduction</h1>
<div id="best-subset-selection" class="section level2">
<h2><span class="header-section-number">7.1</span> Best subset selection</h2>
<p>Algorithm 6.1 from ISLR</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(M_0\)</span> denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.</li>
<li>For k in 1,2,โฆp:
<ol style="list-style-type: lower-alpha">
<li>Fit all p models that contain exactly k predictors. <span class="math inline">\(\left(\begin{array}{c}k\\p\end{array}\right)\)</span></li>
<li>Pick the best among these k models, and call it <span class="math inline">\(M_k\)</span>. Here best is defined as having the smallest RSS, or equivalently largest R<span class="math inline">\(^2\)</span>.</li>
</ol></li>
<li>Select a single best model from among <span class="math inline">\(M_0, \ldots , M_p\)</span> using cross- validated prediction error, C<span class="math inline">\(_p\)</span>, AIC, BIC, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<div class="figure"><span id="fig:unnamed-chunk-77"></span>
<img src="Images/Chapter-6/6.1.png" alt="ISLR Figure 6.1.  The models from best subset selection" width="400" />
<p class="caption">
Figure 7.1: ISLR Figure 6.1. The models from best subset selection
</p>
</div>
</div>
<div id="approximation-to-best-subset-selection" class="section level2">
<h2><span class="header-section-number">7.2</span> Approximation to best subset selection</h2>
<p>For large <span class="math inline">\(p\)</span>, there are too many possible models to fit all of them: <span class="math inline">\(2^p\)</span>. So, some heuristics.</p>
<ol style="list-style-type: decimal">
<li>There are only <span class="math inline">\(p\)</span> models with just one term: <span class="math inline">\(d = 1\)</span>. So easy to try all of them.</li>
<li>There are only <span class="math inline">\(p(p-1)/2\)</span> models with just two terms <span class="math inline">\(d = 2\)</span>. Again, easy to try all of them.</li>
<li>Starting out from <span class="math inline">\(M_1\)</span> or <span class="math inline">\(M_2\)</span>, keep all of those terms and look for the best individual term to add. There will be only <span class="math inline">\(p-(d-1)\)</span> of them. We presume that one of these will be near the frontier of the best possible model with <span class="math inline">\(d\)</span> terms.</li>
<li>Repeat the process <span class="math inline">\(k \gg p\)</span> times, moving forward and back randomly, adding a term or deleting a term.</li>
<li>This will take roughly <span class="math inline">\(k p\)</span> fits <span class="math inline">\(\alpha p^2\)</span> fits, where <span class="math inline">\(\alpha\)</span> is a constant <span class="math inline">\(k/p\)</span>, say 10.</li>
<li>Compare this to <span class="math inline">\(2^p\)</span>. Setting <span class="math inline">\(\alpha = 10\)</span>, find the ratio <span class="math inline">\(\frac{2^p}{\alpha p^2}\)</span> for <span class="math inline">\(p = 5, \ldots, 40\)</span>.</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p &lt;-<span class="st"> </span><span class="dv">1</span>:<span class="dv">40</span>
(<span class="dv">2</span>^p) /<span class="st"> </span>(<span class="dv">10</span> *<span class="st"> </span>p^<span class="dv">2</span>)</code></pre></div>
<pre><code>##  [1] 2.000000e-01 1.000000e-01 8.888889e-02 1.000000e-01 1.280000e-01
##  [6] 1.777778e-01 2.612245e-01 4.000000e-01 6.320988e-01 1.024000e+00
## [11] 1.692562e+00 2.844444e+00 4.847337e+00 8.359184e+00 1.456356e+01
## [16] 2.560000e+01 4.535363e+01 8.090864e+01 1.452321e+02 2.621440e+02
## [21] 4.755447e+02 8.665917e+02 1.585748e+03 2.912711e+03 5.368709e+03
## [26] 9.927347e+03 1.841121e+04 3.423922e+04 6.383721e+04 1.193046e+05
## [31] 2.234634e+05 4.194304e+05 7.887911e+05 1.486148e+06 2.804877e+06
## [36] 5.302429e+06 1.003937e+07 1.903587e+07 3.614437e+07 6.871948e+07</code></pre>
<p>Exhaustion seems find up through about <span class="math inline">\(d = 20\)</span> โ only 100 times more expensive than the random search.</p>
</div>
<div id="classical-theory-of-best-model-choice" class="section level2">
<h2><span class="header-section-number">7.3</span> Classical theory of best model choice</h2>
<p>We <em>punish</em> models with lots of terms.</p>
<table style="width:58%;">
<colgroup>
<col width="16%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th>In-sample</th>
<th>Adjusted</th>
<th>Out-of-sample</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\frac{1}{n}\)</span>RSS</td>
<td><span class="math inline">\(C_p = \frac{1}{n}(\mbox{RSS} + 2 d \hat{\sigma}^2)\)</span></td>
<td>cross-validated prediction error</td>
</tr>
<tr class="even">
<td>.</td>
<td><span class="math inline">\(\mbox{AIC} = -2 \ln {\cal L} - 2 d\)</span></td>
<td>.</td>
</tr>
<tr class="odd">
<td>.</td>
<td><span class="math inline">\(\mbox{AIC}_{ls} = \frac{1}{\hat{\sigma}^2}C_p\)</span></td>
<td>.</td>
</tr>
<tr class="even">
<td>.</td>
<td><span class="math inline">\(\mbox{BIC} = \frac{1}{n} (\mbox{RSS} + \ln(n) d \hat{\sigma}^2)\)</span></td>
<td>.</td>
</tr>
<tr class="odd">
<td>R<span class="math inline">\(^2\)</span></td>
<td>Adjusted R<span class="math inline">\(^2\)</span></td>
<td>???</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(\mbox{Adjusted R}^2 = 1 - \frac{\mbox{RSS}/(n-d-1)}{\mbox{TSS}{(n-1)}}\)</span></p>
<div class="figure"><span id="fig:unnamed-chunk-79"></span>
<img src="Images/Chapter-6/6.2.png" alt="ISLR Figure 6.2.  Note that the values on the vertical axis are the best for that &quot;number of predictors." width="400" />
<p class="caption">
Figure 7.2: ISLR Figure 6.2. Note that the values on the vertical axis are the best for that โnumber of predictors.
</p>
</div>
<p><strong>Uncertainty</strong></p>
<p>Repeat the analysis for different test sets or using different folds in k-fold cross validation.</p>
<ul>
<li>At each value of โNumber of Predictorsโ, there will be a distribution.</li>
<li><em>One-standard-error rule</em>: select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.</li>
</ul>
</div>
<div id="optimization" class="section level2">
<h2><span class="header-section-number">7.4</span> Optimization</h2>
<div id="what-are-we-optimizing-over" class="section level3">
<h3><span class="header-section-number">7.4.1</span> What are we optimizing over?</h3>
<p>Choose the best set of columns from the model matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Small &lt;-<span class="st"> </span>mosaic::<span class="kw">sample</span>(mosaicData::KidsFeet, <span class="dt">size =</span> <span class="dv">5</span>)
<span class="kw">row.names</span>(Small) &lt;-<span class="st"> </span><span class="ot">NULL</span>
M1 &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(width ~<span class="st"> </span>length +<span class="st"> </span>sex, <span class="dt">data =</span> Small)
M2 &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(width ~<span class="st"> </span>length +<span class="st"> </span>sex*biggerfoot, <span class="dt">data =</span> Small)
M3 &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(width ~<span class="st"> </span>length*domhand*sex +<span class="st"> </span>sex*biggerfoot, <span class="dt">data =</span> Small)</code></pre></div>
</div>
</div>
<div id="shrinkage-methods" class="section level2">
<h2><span class="header-section-number">7.5</span> Shrinkage methods</h2>
<p>Example, a roughly quadratic cloud of points. Better to fit it with a constant, straight line, a quadratic?</p>
<ul>
<li>Depends on the amount of data.</li>
<li>What if you have only n=3 or 4?</li>
</ul>
<p>Constant will have the least sampling variation but the most bias.</p>
<p>Quadratic will have the least bias but the most sampling variation.</p>
<p><strong>Shrinkage idea</strong>: Make a linear combination of the constant model with the โfullโ model.</p>
<div id="ridge-regression" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Ridge regression</h3>
<p>The objective function: minimize</p>
<p><span class="math display">\[\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij}\right)^2 + \lambda \sum_{j=1}^p \beta_j^2\]</span></p>
<div class="figure"><span id="fig:unnamed-chunk-81"></span>
<img src="Images/Chapter-6/6.4.png" alt="ISLR Figure 6.4." width="400" />
<p class="caption">
Figure 7.3: ISLR Figure 6.4.
</p>
</div>
<p>Note the y-axis label: โStandardized Coefficients.โ</p>
<ul>
<li>Remember that coefficients have units that depend on the response variable and the explanatory variable(s) participating in the coefficient.</li>
<li>Those coefficients will typically be different from model term to model term.</li>
<li>Meaningless to add up numbers with different physical dimension.</li>
</ul>
<p>โฆ So, standardize the explanatory variables.</p>
<p>Another perspective on the reason to standardize: Some of the coefficients might be huge numerically, others small. In such a situation, the huge coefficients will dominate; the small ones will have little influence on the shrinkage.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(CPS85, <span class="dt">package =</span> <span class="st">&quot;mosaic&quot;</span>)</code></pre></div>
<pre><code>## Warning in data(CPS85, package = &quot;mosaic&quot;): data set &#39;CPS85&#39; not found</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(wage ~<span class="st"> </span>age +<span class="st"> </span>educ +<span class="st"> </span>sex +<span class="st"> </span>union +<span class="st"> </span>married, <span class="dt">data =</span> CPS85)
foo &lt;-<span class="st"> </span><span class="kw">glmnet</span>(x, CPS85$wage, <span class="dt">alpha =</span> <span class="dv">0</span>)
foo$lambda.min</code></pre></div>
<pre><code>## NULL</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(foo,<span class="dt">s=</span>foo$lambda.min,<span class="dt">exact=</span>T,<span class="dt">type=</span><span class="st">&quot;coefficients&quot;</span>)</code></pre></div>
<pre><code>## 7 x 100 sparse Matrix of class &quot;dgCMatrix&quot;</code></pre>
<pre><code>##    [[ suppressing 100 column names &#39;s0&#39;, &#39;s1&#39;, &#39;s2&#39; ... ]]</code></pre>
<pre><code>##                                                                      
## (Intercept)    9.024064e+00  8.9845500258  8.9807084640  8.9764945908
## (Intercept)    .             .             .             .           
## age            7.833801e-38  0.0002223183  0.0002439361  0.0002676496
## educ           7.580412e-37  0.0021509145  0.0023600259  0.0025894031
## sexM           2.137431e-36  0.0060625175  0.0066516621  0.0072978516
## unionUnion     2.184744e-36  0.0061916460  0.0067927982  0.0074520479
## marriedSingle -1.097614e-36 -0.0031093285 -0.0034110717 -0.0037419469
##                                                                     
## (Intercept)    8.971872561  8.9668031280  8.9612433278  8.9551461420
## (Intercept)    .            .             .             .           
## age            0.000293661  0.0003221914  0.0003534828  0.0003878003
## educ           0.002840997  0.0031169438  0.0034195819  0.0037514703
## sexM           0.008006567  0.0087838090  0.0096361411  0.0105707449
## unionUnion     0.008174955  0.0089676003  0.0098366324  0.0107893174
## marriedSingle -0.004104736 -0.0045024806 -0.0049385043 -0.0054164378
##                                                                     
## (Intercept)    8.9484601281  8.9411290199  8.933091297  8.9242797166
## (Intercept)    .             .             .            .           
## age            0.0004254338  0.0004667008  0.000511948  0.0005615549
## educ           0.0041154088  0.0045144596  0.004951971  0.0054316019
## sexM           0.0115954738  0.0127189125  0.013950443  0.0153003109
## unionUnion     0.0118335936  0.0129781289  0.014232384  0.0156066793
## marriedSingle -0.0059402456 -0.0065142539 -0.007143182 -0.0078321724
##                                                                      
## (Intercept)    8.9146208149  8.9040343629  8.8924327880  8.8797205546
## (Intercept)    .             .             .             .           
## age            0.0006159361  0.0006755443  0.0007408743  0.0008124655
## educ           0.0059573508  0.0065335834  0.0071650649  0.0078569941
## sexM           0.0167797050  0.0184008330  0.0201770102  0.0221227462
## unionUnion     0.0171122647  0.0187613975  0.0205674209  0.0225448498
## marriedSingle -0.0085868290 -0.0094132503 -0.0103180689 -0.0113084920
##                                                                   
## (Intercept)    8.8657934983  8.8505381222  8.833830848  8.81553723
## (Intercept)    .             .             .            .         
## age            0.0008909068  0.0009768399  0.001070964  0.00117404
## educ           0.0086150384  0.0094453731  0.010354721  0.01135040
## sexM           0.0242538476  0.0265875170  0.029142461  0.03193901
## unionUnion     0.0247094586  0.0270783749  0.029670175  0.03250498
## marriedSingle -0.0123923429 -0.0135781042 -0.014874962 -0.01629285
##                                                                  
## (Intercept)    8.795511112  8.773593781  8.749613049  8.723382330
## (Intercept)    .            .            .            .          
## age            0.001286896  0.001410431  0.001545623  0.001693531
## educ           0.012440352  0.013633217  0.014938358  0.016365920
## sexM           0.034999211  0.038346996  0.042008261  0.046011017
## unionUnion     0.035604563  0.038992442  0.042693981  0.046736492
## marriedSingle -0.017842503 -0.019535481 -0.021384232 -0.023402122
##                                                                 
## (Intercept)    8.694699697  8.66334692  8.629088521  8.591670836
## (Intercept)    .            .           .            .          
## age            0.001855303  0.00203218  0.002225506  0.002436727
## educ           0.017926885  0.01963311  0.021497409  0.023533552
## sexM           0.050385511  0.05516435  0.060382633  0.066078045
## unionUnion     0.051149320  0.05596393  0.061213970  0.066935341
## marriedSingle -0.025603471 -0.02800358 -0.030618769 -0.033466351
##                                                                  
## (Intercept)    8.550821131  8.506246777  8.457634524  8.404649905
## (Intercept)    .            .            .            .          
## age            0.002667402  0.002919208  0.003193942  0.003493528
## educ           0.025756363  0.028181734  0.030826673  0.033709334
## sexM           0.072290980  0.079064621  0.086445009  0.094481084
## unionUnion     0.073166217  0.079947059  0.087320592  0.095331734
## marriedSingle -0.036564669 -0.039933055 -0.043591796 -0.047562072
##                                                                 
## (Intercept)    8.346936807  8.284117277  8.21579158  8.141538616
## (Intercept)    .            .            .           .          
## age            0.003820021  0.004175608  0.00456261  0.004983484
## educ           0.036849035  0.040266269  0.04398269  0.048021111
## sexM           0.103224702  0.112730595  0.12305630  0.134262014
## unionUnion     0.104027489  0.113456777  0.12367019  0.134719696
## marriedSingle -0.051865865 -0.056525832 -0.06156514 -0.067007279
##                                                                  
## (Intercept)    8.060916717  7.973464940  7.878704903  7.776143265
## (Intercept)    .            .            .            .          
## age            0.005440816  0.005937322  0.006475838  0.007059311
## educ           0.052405411  0.057160500  0.062312194  0.067887072
## sexM           0.146410399  0.159566289  0.173796323  0.189168465
## unionUnion     0.146658213  0.159539140  0.173415734  0.188340405
## marriedSingle -0.072875762 -0.079193859 -0.085984206 -0.093268375
##                                                                           
## (Intercept)    7.665274929  7.54558705  7.416563927  7.27769283  7.1284709
## (Intercept)    .            .           .            .           .        
## age            0.007690785  0.00837338  0.009110276  0.00990468  0.0107598
## educ           0.073912294  0.08041537  0.087423891  0.09496517  0.1030659
## sexM           0.205751424  0.22361395  0.242824003  0.26344778  0.2855486
## unionUnion     0.204363867  0.22153418  0.239895671  0.25948770  0.2803434
## marriedSingle -0.101066377 -0.10939610 -0.118272683 -0.12770782 -0.1377091
##                                                                         
## (Intercept)    6.96841279  6.79705996  6.61399012  6.41882823  6.2112579
## (Intercept)    .           .           .           .           .        
## age            0.01167879  0.01266472  0.01372055  0.01484901  0.0160526
## educ           0.11175163  0.12104636  0.13097192  0.14154739  0.1527884
## sexM           0.30918583  0.33441321  0.36127774  0.38981798  0.4200625
## unionUnion     0.30248814  0.32593832  0.35069962  0.37676565  0.4041166
## marriedSingle -0.14827895 -0.15941437 -0.17110566 -0.18333586 -0.1960801
##                                                                        
## (Intercept)    5.9910334  5.7579921  5.51206649  5.25329617  4.98183860
## (Intercept)    .          .          .           .           .         
## age            0.0173335  0.0186935  0.02013392  0.02165556  0.02325861
## educ           0.1647066  0.1773089  0.19059658  0.20456512  0.21920324
## sexM           0.4520282  0.4857189  0.52112390  0.55821629  0.59695227
## unionUnion     0.4327179  0.4625192  0.49345370  0.52543758  0.55837010
## marriedSingle -0.2093050 -0.2229683 -0.23701831 -0.25139453 -0.26602738
##                                                                          
## (Intercept)    4.69797856  4.40213581  4.09487021  3.77688286  3.44901969
## (Intercept)    .           .           .           .           .         
## age            0.02494258  0.02670626  0.02854762  0.03046385  0.03245118
## educ           0.23449256  0.25040721  0.26691358  0.28397034  0.30152843
## sexM           0.63727009  0.67908961  0.72231213  0.76682031  0.81248048
## unionUnion     0.59213404  0.62659667  0.66161129  0.69701946  0.73265266
## marriedSingle -0.28083903 -0.29574427 -0.31065174 -0.32546539 -0.34008662
##                                                                          
## (Intercept)    3.11226065  2.76771551  2.41661109  2.06027583  1.70012110
## (Intercept)    .           .           .           .           .         
## age            0.03450504  0.03661995  0.03878962  0.04100694  0.04326407
## educ           0.31953158  0.33791673  0.35661486  0.37555188  0.39464982
## sexM           0.85914169  0.90663875  0.95479401  1.00341984  1.05232141
## unionUnion     0.76833644  0.80389287  0.83914426  0.87391666  0.90804331
## marriedSingle -0.35441592 -0.36835537 -0.38181085 -0.39469431 -0.40692595
##                                                                          
## (Intercept)    1.33761998  0.97428376  0.61163733  0.25119401 -0.10556916
## (Intercept)    .           .           .           .           .         
## age            0.04555258  0.04786348  0.05018744  0.05251487  0.05483612
## educ           0.41382804  0.43300458  0.45209753  0.47102648  0.48971376
## sexM           1.10129972  1.15015470  1.19868837  1.24670797  1.29402881
## unionUnion     0.94136790  0.97374751  1.00505497  1.03518081  1.06403449
## marriedSingle -0.41843607 -0.42916675 -0.43907299 -0.44812347 -0.45630085
##                                                                          
## (Intercept)   -0.45723496 -0.80246738 -1.14003052 -1.46880417 -1.78779598
## (Intercept)    .           .           .           .           .         
## age            0.05714158  0.05942192  0.06166817  0.06387188  0.06602526
## educ           0.50808576  0.52607395  0.54361585  0.56065574  0.57714518
## sexM           1.34047696  1.38589155  1.43012672  1.47305307  1.51455874
## unionUnion     1.09154508  1.11766129  1.14235091  1.16559986  1.18741066
## marriedSingle -0.46360157 -0.47003518 -0.47562331 -0.48039836 -0.48440181
##                                                                         
## (Intercept)   -2.09614984 -2.39315067 -2.6782256 -2.95094196 -3.21100222
## (Intercept)    .           .           .          .           .         
## age            0.06812123  0.07015356  0.0721169  0.07400677  0.07581963
## educ           0.59304334  0.60831709  0.6229410  0.63689689  0.65017384
## sexM           1.55455004  1.59295153  1.6297059  1.66477322  1.69813027
## unionUnion     1.20780080  1.22680071  1.2444518  1.26080428  1.27591536
## marriedSingle -0.48768253 -0.49029496 -0.4922973 -0.49374981 -0.49471329
##                                                                          
## (Intercept)   -3.45823680 -3.69259470 -3.91413280 -4.12300419 -4.31944604
## (Intercept)    .           .           .           .           .         
## age            0.07755284  0.07920461  0.08077398  0.08226072  0.08366532
## educ           0.66276734  0.67467888  0.68591532  0.69648818  0.70641303
## sexM           1.72976918  1.75969624  1.78793039  1.81450170  1.83944980
## unionUnion     1.28984720  1.30266526  1.31443681  1.32522960  1.33511079
## marriedSingle -0.49524770 -0.49541101 -0.49525830 -0.49484107 -0.49420673
##                                                                          
## (Intercept)   -4.50376732 -4.67633677 -4.83757141 -4.98792574 -5.12788178
## (Intercept)    .           .           .           .           .         
## age            0.08498886  0.08623295  0.08739966  0.08849141  0.08951095
## educ           0.71570878  0.72439708  0.73250174  0.74004811  0.74706270
## sexM           1.86282238  1.88467367  1.90506303  1.92405372  1.94171161
## unionUnion     1.34414606  1.35239889  1.35993009  1.36679741  1.37305531
## marriedSingle -0.49339833 -0.49245445 -0.49140918 -0.49029227 -0.48912933
##                                                                        
## (Intercept)   -5.25794016 -5.3786121 -5.49041268 -5.59385451 -5.6894432
## (Intercept)    .           .          .           .           .        
## age            0.09046123  0.0913454  0.09216668  0.09292837  0.0936338
## educ           0.75357262  0.7596053  0.76518802  0.77034787  0.7751113
## sexM           1.95810424  1.9732998  1.98736634  2.00037119  2.0123803
## unionUnion     1.37875481  1.3839435  1.38866554  1.39296178  1.3968698
## marriedSingle -0.48794212 -0.4867489 -0.48556453 -0.48440129 -0.4832687
##                                                                          
## (Intercept)   -5.77767290 -5.85902333 -5.93390991 -6.00287296 -6.06628568
## (Intercept)    .           .           .           .           .         
## age            0.09428624  0.09488895  0.09544397  0.09595664  0.09642875
## educ           0.77950393  0.78355064  0.78727494  0.79070015  0.79384759
## sexM           2.02345769  2.03366541  2.04306574  2.05170968  2.05965435
## unionUnion     1.40042429  1.40365684  1.40658991  1.40926359  1.41169502
## marriedSingle -0.48217421 -0.48112327 -0.48012751 -0.47917354 -0.47827092</code></pre>
<p>For <code>Credit</code> dataset: <a href="http://www-bcf.usc.edu/~gareth/ISL/Credit.csv" class="uri">http://www-bcf.usc.edu/~gareth/ISL/Credit.csv</a></p>
</div>
</div>
<div id="lasso" class="section level2">
<h2><span class="header-section-number">7.6</span> LASSO</h2>
<p>The objective function: minimize</p>
<p><span class="math display">\[\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij}\right)^2 + \lambda \sum_{j=1}^p |\beta_i|\]</span> Almost exactly the same as ridge regression. But the small change makes a big difference.</p>
<div class="figure"><span id="fig:unnamed-chunk-83"></span>
<img src="Images/Chapter-6/6.7.png" alt="ISLR Figure 6.7." width="100%" />
<p class="caption">
Figure 7.4: ISLR Figure 6.7.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(wage ~<span class="st"> </span>age *<span class="st"> </span>educ *<span class="st"> </span>sex *<span class="st"> </span>union *<span class="st"> </span>married, <span class="dt">data =</span> CPS85)
foo &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(x, CPS85$wage, <span class="dt">alpha =</span> <span class="dv">1</span>)
foo$lambda.min</code></pre></div>
<pre><code>## [1] 0.1263735</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(foo,<span class="dt">s=</span>foo$lambda.min,<span class="dt">exact=</span>T,<span class="dt">type=</span><span class="st">&quot;coefficients&quot;</span>)</code></pre></div>
<pre><code>## 33 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                                                    1
## (Intercept)                            -1.9708098459
## (Intercept)                             .           
## age                                     .           
## educ                                    0.5482123152
## sexM                                    .           
## unionUnion                              1.1057230664
## marriedSingle                           .           
## age:educ                                0.0054884080
## age:sexM                                0.0559924024
## educ:sexM                               .           
## age:unionUnion                          .           
## educ:unionUnion                         .           
## sexM:unionUnion                         .           
## age:marriedSingle                       .           
## educ:marriedSingle                      .           
## sexM:marriedSingle                      .           
## unionUnion:marriedSingle                .           
## age:educ:sexM                           0.0003101264
## age:educ:unionUnion                     .           
## age:sexM:unionUnion                     .           
## educ:sexM:unionUnion                    .           
## age:educ:marriedSingle                  .           
## age:sexM:marriedSingle                  .           
## educ:sexM:marriedSingle                -0.0453647898
## age:unionUnion:marriedSingle            .           
## educ:unionUnion:marriedSingle           .           
## sexM:unionUnion:marriedSingle           .           
## age:educ:sexM:unionUnion                .           
## age:educ:sexM:marriedSingle             .           
## age:educ:unionUnion:marriedSingle      -0.0002003640
## age:sexM:unionUnion:marriedSingle       .           
## educ:sexM:unionUnion:marriedSingle      .           
## age:educ:sexM:unionUnion:marriedSingle  .</code></pre>
</div>
<div id="review" class="section level2">
<h2><span class="header-section-number">7.7</span> Review</h2>
<p>We started by considering one mathematical representation of the problem: Choose <span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_p\)</span> to minimize this <em>objective function</em></p>
<p><span class="math display">\[\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij}\right)^2 + \lambda \sum_{j=1}^p \beta_j^2\]</span> The result depends on the value of <span class="math inline">\(\lambda\)</span>, which is often called the <em>Lagrange multiplier</em>.</p>
<p>The motivation for this form of the objective function is rooted in two different and conflicting goals:</p>
<ol style="list-style-type: decimal">
<li>Get the model as close as possible to the data: minimize the sum of square errors.</li>
<li>Keep the <span class="math inline">\(\beta\)</span>โs small: minimize the model variance.</li>
</ol>
<p><em>Reductio ad absurdum</em> argument. Suppose <span class="math inline">\(\lambda \rightarrow \infty\)</span>. Then the best <span class="math inline">\(\beta\)</span>โs would be zeros. Such a model has zero model variance, since weโll get exactly the same result on every data set used to train the model.</p>
<p>Another way to think about multi-objective optimization โฆ</p>
<p>We have several goals as a function of the adjustable parameters. Letโs denote them by <span class="math inline">\(g_1(x), g_2(x), g_3(x), \ldots\)</span>. For instance, suppose <span class="math inline">\(g_1(x)\)</span> is the appeal of a meal as a function of the ingredients, <span class="math inline">\(g_2(x)\)</span> is the vitamin content, <span class="math inline">\(g_3(x)\)</span> is the protein content, <span class="math inline">\(g_4(x)\)</span> is the amount of salt, and so on.</p>
<p>We want to find the parameters <span class="math inline">\(x\)</span> that minimize some mixture of the objectives, e.g.</p>
<p><span class="math display">\[T(x; \lambda_1, \lambda_2, \ldots) \equiv \lambda_1 g_1(x) + \lambda_2 g_2(x) + \ldots\]</span> We donโt know what the <span class="math inline">\(\lambda\)</span>โs are; they reflect the relative importance of the different components of the objective function. We could, of course, make them up. But instead โฆ</p>
<ol style="list-style-type: decimal">
<li>Letโs choose one of the components to be our sole objective. It can be any of the components, the end result will be the same regardless. The others we will consider as <em>constraints</em>. For instance, we might take <em>taste</em> (<span class="math inline">\(g_1(x)\)</span>) as our objective but impose constraints: calories should be between 800 and 1200, salt should be below 1200 mg, protein should be above 50 grams.</li>
<li>Consider all the values of <span class="math inline">\(x\)</span> that satisfy the constraints. Search over these to find the one that maximizes our selected objective component. This is called <em>constrained optimization</em>.</li>
<li>Consider each of the constraints. Find out how much the selected objective will improve if we relax that constraint a bit, say allowing salt to be 1300 mg. The ratio is the Lagrange multiplier <span class="math inline">\(\lambda_i\)</span>.</li>
</ol>
<p><span class="math display">\[\frac{\mbox{change in taste}}{\mbox{change in constraint}_i} \equiv \lambda_i\]</span></p>
<p>Now ask questions about priorities: If I were to ease up on the constraint, would I get enough change in the objective to be worthwhile? Or, if I were to tighten the constraint, would the resulting change in objective be acceptable? Change the constraints accordingly until you no longer see a net gain by changing the constraints.</p>
<p>Letโs translate this to ridge and lasso regression.</p>
<ol style="list-style-type: decimal">
<li>The selected component of the overall objective: Minimize the in-sample sum of square error.</li>
<li>The constraint: how big are we willing to make the coefficients? (Remember, by standardizing the variables we make it possible to calculate a total size.)</li>
<li>Evaluating the trade-offs between constraints and objectives: does the out-of-sample prediction error get smaller.</li>
</ol>
<p>The picture:</p>
<ul>
<li>the red shows the objective function</li>
<li>the blue shows the permitted region for the coefficients to satisfy the constraint.</li>
</ul>
<div class="figure"><span id="fig:unnamed-chunk-85"></span>
<img src="Images/Chapter-6/6.7.png" alt="ISLR Figure 6.7." width="400" />
<p class="caption">
Figure 7.5: ISLR Figure 6.7.
</p>
</div>
<p>We can imagine increasing or decreasing the constraints, and will get a different optimum at each level of the constraint.</p>
<p>Why the circle and diamond?</p>
<p>Three different โmetrics,โ that is three different ways of combining parts to get an overall size.</p>
<ul>
<li><span class="math inline">\(L^2\)</span> โ square-root of the sum of squares. The usual Euclidean distance. Circle</li>
<li><span class="math inline">\(L^1\)</span> โ sum of the absolute values. Sometimes called the Manhattan metric. Diamond</li>
<li><span class="math inline">\(L^\infty\)</span> โ the biggest individual value. Square.</li>
</ul>
<p>Predicting <code>Salary</code> in the <code>ISLR::Hitters</code> data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Without_NA &lt;-<span class="st"> </span><span class="kw">na.omit</span>(ISLR::Hitters)
inds &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(Without_NA), <span class="dt">size =</span> <span class="kw">nrow</span>(Without_NA)/<span class="dv">2</span>)
Train &lt;-<span class="st"> </span>Without_NA[inds,]
Test &lt;-<span class="st"> </span>Without_NA[-inds,]
y_all &lt;-<span class="st"> </span>Without_NA$Salary
x_all &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(Salary ~<span class="st"> </span>., <span class="dt">data=</span>Without_NA)
y_train &lt;-<span class="st"> </span>Train$Salary
x_train &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(Salary ~<span class="st"> </span>., <span class="dt">data=</span>Train)
y_test &lt;-<span class="st"> </span>Test$Salary
x_test &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(Salary ~<span class="st"> </span>., <span class="dt">data=</span>Test)
ridge_mod &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(x_train, y_train, <span class="dt">alpha =</span> <span class="dv">0</span>)
ridge_mod$lambda.min
ridge_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(ridge_mod, <span class="dt">s=</span><span class="dv">0</span>, <span class="dt">newx =</span> x_test, <span class="dt">exact=</span><span class="ot">TRUE</span>)
<span class="kw">mean</span>((ridge_pred -<span class="st"> </span>y_test)^<span class="dv">2</span>)
final &lt;-<span class="st"> </span><span class="kw">glmnet</span>(x_all, y_all, <span class="dt">alpha=</span><span class="dv">0</span>)
<span class="kw">predict</span>(final, <span class="dt">type=</span><span class="st">&quot;coefficients&quot;</span>, <span class="dt">s=</span>ridge_mod$lambda.min)</code></pre></div>
<p>Lasso: Do we really need all of those variables?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lasso_mod &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(x_train, y_train, <span class="dt">alpha =</span> <span class="dv">1</span>)
lasso_mod$lambda.min
lasso_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(lasso_mod, <span class="dt">s=</span><span class="dv">0</span>, <span class="dt">newx =</span> x_test, <span class="dt">exact=</span><span class="ot">TRUE</span>)
<span class="kw">mean</span>((lasso_pred -<span class="st"> </span>y_test)^<span class="dv">2</span>)
final &lt;-<span class="st"> </span><span class="kw">glmnet</span>(x_all, y_all, <span class="dt">alpha=</span><span class="dv">1</span>)
<span class="kw">predict</span>(final, <span class="dt">type=</span><span class="st">&quot;coefficients&quot;</span>, <span class="dt">s=</span>lasso_mod$lambda.min)</code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-88"></span>
<img src="Images/Chapter-6/6.4.png" alt="ISLR Figure 6.4" width="400" />
<p class="caption">
Figure 7.6: ISLR Figure 6.4
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-89"></span>
<img src="Images/Chapter-6/6.7.png" alt="ISLR Figure 6.7" width="400" />
<p class="caption">
Figure 7.7: ISLR Figure 6.7
</p>
</div>
</div>
<div id="multi-collinearity" class="section level2">
<h2><span class="header-section-number">7.8</span> Multi-collinearity</h2>
<p>The SAT story.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">lm</span>(sat ~<span class="st"> </span>expend, <span class="dt">data=</span>mosaicData::SAT))$coef</code></pre></div>
<pre><code>##               Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 1089.29372  44.389950 24.539197 8.168276e-29
## expend       -20.89217   7.328209 -2.850925 6.407965e-03</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">lm</span>(sat ~<span class="st"> </span>expend +<span class="st"> </span>ratio, <span class="dt">data=</span>mosaicData::SAT))$coef</code></pre></div>
<pre><code>##                Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept) 1136.335547 107.803485 10.5408053 5.693212e-14
## expend       -22.307944   7.955544 -2.8040751 7.313013e-03
## ratio         -2.294539   4.783836 -0.4796442 6.337049e-01</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">lm</span>(sat ~<span class="st"> </span>expend +<span class="st"> </span>ratio +<span class="st"> </span>salary, <span class="dt">data=</span>mosaicData::SAT))$coef</code></pre></div>
<pre><code>##                Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept) 1069.234168 110.924940  9.6392585 1.292219e-12
## expend        16.468866  22.049899  0.7468907 4.589302e-01
## ratio          6.330267   6.542052  0.9676272 3.382908e-01
## salary        -8.822632   4.696794 -1.8784372 6.666771e-02</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mosaic::<span class="kw">rsquared</span>(<span class="kw">lm</span>(expend ~<span class="st"> </span>ratio +<span class="st"> </span>salary, <span class="dt">data=</span>mosaicData::SAT))</code></pre></div>
<pre><code>## [1] 0.893476</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-91"></span>
<img src="Images/CI-1.png" alt="Confidence interval explanation" width="356" />
<p class="caption">
Figure 7.8: Confidence interval explanation
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-92"></span>
<img src="Images/CI-2.png" alt="Confidence interval explanation, part 2" width="722" />
<p class="caption">
Figure 7.9: Confidence interval explanation, part 2
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;Images/mona.rda&quot;</span>)
<span class="kw">rankMatrix</span>(mona)</code></pre></div>
<pre><code>## [1] 191
## attr(,&quot;method&quot;)
## [1] &quot;tolNorm2&quot;
## attr(,&quot;useGrad&quot;)
## [1] FALSE
## attr(,&quot;tol&quot;)
## [1] 5.551115e-14</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># pick a vector at random; column 151 versus the first 131 columns</span>
mosaic::<span class="kw">rsquared</span>(<span class="kw">lm</span>(<span class="kw">t</span>(mona)[,<span class="dv">151</span>] ~<span class="st"> </span><span class="kw">t</span>(mona)[,-<span class="dv">151</span>]))</code></pre></div>
<pre><code>## [1] 0.9999636</code></pre>
<p>Variance inflation factor</p>
<p><span class="math inline">\(\mbox{VIF}(\beta_j) = \frac{1}{1 - R^2_{x_j|x_{-j}}}\)</span></p>
<p>Getting rid of vectors that correlate substantially with one another can reduce the variance inflation factor.</p>
</div>
<div id="creating-correlations" class="section level2">
<h2><span class="header-section-number">7.9</span> Creating correlations</h2>
<p>Generate points on circles of radius 1, 2, 3, โฆ</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">make_circles &lt;-<span class="st"> </span>function(<span class="dt">radii =</span> <span class="dv">1</span>:<span class="dv">2</span>, <span class="dt">nangs =</span> <span class="dv">30</span>) {
  theta =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">2</span>*pi, <span class="dt">length =</span> nangs)
  x &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">cos</span>(theta), <span class="kw">length</span>(radii))
  y &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">sin</span>(theta), <span class="kw">length</span>(radii))
  r &lt;-<span class="st"> </span><span class="kw">rep</span>(radii, <span class="dt">each =</span> nangs)
  col &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">rainbow</span>(nangs), <span class="kw">length</span>(radii))
  <span class="kw">data.frame</span>(<span class="dt">x =</span> x *<span class="st"> </span>r, <span class="dt">y =</span> y *<span class="st"> </span>r, <span class="dt">r =</span> r, <span class="dt">col =</span> col)
}
transform_circles &lt;-<span class="st"> </span>function(M, <span class="dt">circles =</span> <span class="ot">NULL</span>) {
  if (<span class="kw">is.null</span>(circles)) circles &lt;-<span class="st"> </span><span class="kw">make_circles</span>()
  XY &lt;-<span class="st"> </span><span class="kw">rbind</span>(circles$x, circles$y)
  new &lt;-<span class="st"> </span>M %*%<span class="st"> </span>XY
  circles$x =<span class="st"> </span>new[<span class="dv">1</span>, ]
  circles$y =<span class="st"> </span>new[<span class="dv">2</span>, ]
  
  circles
}



Trans &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, -<span class="dv">3</span>, -<span class="dv">1</span>), <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
After_trans &lt;-<span class="st"> </span><span class="kw">transform_circles</span>(Trans)
<span class="kw">plot</span>(y ~<span class="st"> </span>x, <span class="dt">data =</span> After_trans, <span class="dt">col =</span> (After_trans$col), <span class="dt">asp =</span> <span class="dv">1</span>, <span class="dt">pch =</span> <span class="dv">20</span>)</code></pre></div>
<p><img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-94-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svals &lt;-<span class="st"> </span><span class="kw">svd</span>(Trans)
Start &lt;-<span class="st"> </span><span class="kw">make_circles</span>()
<span class="kw">plot</span>(y ~<span class="st"> </span>x, <span class="dt">data =</span> Start, <span class="dt">col =</span> Start$col, <span class="dt">asp =</span> <span class="dv">1</span>, <span class="dt">pch =</span> <span class="dv">20</span>)</code></pre></div>
<p><img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-94-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">After_V &lt;-<span class="st"> </span><span class="kw">transform_circles</span>(<span class="kw">t</span>(svals$v), Start)
<span class="kw">plot</span>(y ~<span class="st"> </span>x, <span class="dt">data =</span> After_V, <span class="dt">col =</span> After_V$col, <span class="dt">asp =</span> <span class="dv">1</span>, <span class="dt">pch =</span> <span class="dv">20</span>)</code></pre></div>
<p><img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-94-3.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">After_V_lambda &lt;-<span class="st"> </span><span class="kw">transform_circles</span>(<span class="kw">diag</span>(svals$d), After_V)
<span class="kw">plot</span>(y ~<span class="st"> </span>x, data &lt;-<span class="st"> </span>After_V_lambda, <span class="dt">col =</span> After_V_lambda$col, <span class="dt">asp =</span> <span class="dv">1</span>, <span class="dt">pch =</span> <span class="dv">20</span>)</code></pre></div>
<p><img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-94-4.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">After_V_lambda_U &lt;-<span class="st"> </span><span class="kw">transform_circles</span>(svals$u, After_V_lambda)
<span class="kw">plot</span>(y ~<span class="st"> </span>x, <span class="dt">data =</span> After_V, <span class="dt">col =</span> After_V_lambda_U$col, <span class="dt">asp =</span> <span class="dv">1</span>, <span class="dt">pch =</span> <span class="dv">20</span>)</code></pre></div>
<p><img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-94-5.png" width="672" /></p>
</div>
<div id="rank-1-matrices" class="section level2">
<h2><span class="header-section-number">7.10</span> Rank 1 Matrices</h2>
<p>Suppose all the columns in a matrix are simple multiples of the first column. All of the columns would be exactly collinear, so <code>lm()</code> will produce <code>NA</code> for all but one of the coefficients (putting the intercept aside). Lasso would zero out all but one of the coefficients, ridge would โฆ</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">101</span>)
one &lt;-<span class="st"> </span><span class="kw">outer</span>(<span class="kw">rnorm</span>(<span class="dv">100</span>), <span class="kw">rnorm</span>(<span class="dv">10</span>))
two &lt;-<span class="st"> </span><span class="kw">outer</span>(<span class="kw">rnorm</span>(<span class="dv">100</span>), <span class="kw">rnorm</span>(<span class="dv">10</span>))
three &lt;-<span class="st"> </span><span class="kw">outer</span>(<span class="kw">rnorm</span>(<span class="dv">100</span>), <span class="kw">rnorm</span>(<span class="dv">10</span>))
y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>) +<span class="st"> </span><span class="dv">7</span> *<span class="st"> </span>one[, <span class="dv">1</span>] -<span class="st"> </span><span class="dv">2</span> *<span class="st"> </span>two[, <span class="dv">1</span>]
x &lt;-<span class="st"> </span>one +<span class="st"> </span>two

<span class="kw">dim</span>(one)</code></pre></div>
<pre><code>## [1] 100  10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lm</span>(y ~<span class="st"> </span>x)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Coefficients:
## (Intercept)           x1           x2           x3           x4  
##    -0.06554    -17.08557    -10.65980           NA           NA  
##          x5           x6           x7           x8           x9  
##          NA           NA           NA           NA           NA  
##         x10  
##          NA</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lasso_mod &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(x, y, <span class="dt">alpha =</span> <span class="dv">1</span>)
lasso_mod$lambda.min</code></pre></div>
<pre><code>## [1] 0.02322399</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(lasso_mod, <span class="dt">type=</span><span class="st">&quot;coefficients&quot;</span>, <span class="dt">s=</span>lasso_mod$lambda.min)</code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                       1
## (Intercept) -0.06467677
## V1           .         
## V2           .         
## V3           .         
## V4           .         
## V5           .         
## V6          -3.00961477
## V7           .         
## V8           .         
## V9          -2.19570742
## V10          .</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ridge_mod &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(one, y, <span class="dt">alpha =</span> <span class="dv">0</span>)
ridge_mod$lambda.min</code></pre></div>
<pre><code>## [1] 2.046288</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(ridge_mod, <span class="dt">type=</span><span class="st">&quot;coefficients&quot;</span>, <span class="dt">s=</span>ridge_mod$lambda.min)</code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                       1
## (Intercept)  0.12330285
## V1           0.52231401
## V2          -0.23757058
## V3           0.06598347
## V4           0.11986807
## V5           0.18788200
## V6          -0.60749645
## V7           1.59250740
## V8          -0.06389472
## V9          -0.29849103
## V10          0.08247090</code></pre>
</div>
<div id="idea-of-singular-values." class="section level2">
<h2><span class="header-section-number">7.11</span> Idea of singular values.</h2>
<p>Find orthogonal vectors to describe the ellipsoidal cloud. The singular value describes โhow longโ each ellipsoidal axis is.</p>
<p>Correlation <span class="math inline">\(R^2_{x_j | x_{-j}}\)</span> gets increased for each <em>direction</em> that overlaps between <span class="math inline">\(x_j\)</span> and <span class="math inline">\(x_{-j}\)</span> โ it doesnโt matter how big the singular value is in that direction. Only by throwing out <em>directions</em> can we reduce <span class="math inline">\(R^2_{x_j | x_{-j}}\)</span></p>
<p>Nice illustration:</p>
<div class="figure">
<img src="Images/Singular-value-picture.png" />

</div>
<p><a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#/media/File:Singular-Value-Decomposition.svg">Source</a></p>
</div>
<div id="dimension-reduction" class="section level2">
<h2><span class="header-section-number">7.12</span> Dimension reduction</h2>
<p>Re-arrange the variables to squeeze the juice out of them.</p>
<ol style="list-style-type: decimal">
<li>Matrix</li>
<li>Approximate matrix in a least squares sense. If that approximation includes the same column or more, we can discard the repeats.</li>
<li>Outer product</li>
<li>Rank-1 matrix constructed by creating multiples of one column.</li>
<li>Create another vector and another rank-1 matrix. Add it up and we get closer to the target.</li>
</ol>
<p>Creating those singular vectors:</p>
<ul>
<li>singular value decomposition</li>
<li><span class="math inline">\({\mathbf D}\)</span> gives information on how big they are</li>
<li>orthogonal to one another</li>
<li>cumulative sum of <span class="math inline">\({\mathbf D}\)</span> components gives the amount of variance in the approximation.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res &lt;-<span class="st"> </span><span class="kw">svd</span>(mona)
approx &lt;-<span class="st"> </span><span class="dv">0</span>
for (i in <span class="dv">1</span>:<span class="dv">10</span>) {
  approx &lt;-<span class="st"> </span>approx +<span class="st"> </span><span class="kw">outer</span>(res$u[,i], res$v[,i]) *<span class="st"> </span>res$d[i]
}
<span class="kw">image</span>(approx, <span class="dt">asp=</span><span class="dv">1</span>)</code></pre></div>
<p><img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-96-1.png" width="672" /></p>
<p>Picture in terms of gaussian cloud. The covariance matrix tells all that you need.</p>
<p>Using <code>pcr()</code> to fit models, interpreting the output.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pls)</code></pre></div>
<pre><code>## 
## Attaching package: &#39;pls&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     loadings</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pcr.fit &lt;-<span class="st"> </span><span class="kw">pcr</span>(Salary ~<span class="st"> </span>., <span class="dt">data =</span> ISLR::Hitters, <span class="dt">scale=</span><span class="ot">TRUE</span>, <span class="dt">validation=</span><span class="st">&quot;CV&quot;</span>)
<span class="kw">summary</span>(pcr.fit)</code></pre></div>
<pre><code>## Data:    X dimension: 263 19 
##  Y dimension: 263 1
## Fit method: svdpc
## Number of components considered: 19
## 
## VALIDATION: RMSEP
## Cross-validated using 10 random segments.
##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
## CV             452    354.3    353.2    353.7    351.6    348.7    345.2
## adjCV          452    353.9    352.7    353.1    350.9    348.0    344.3
##        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
## CV       345.8    349.3    351.5     357.6     358.9     361.6     362.3
## adjCV    344.9    348.2    350.3     356.0     357.1     359.8     360.4
##        14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
## CV        355.4       355     346.7     346.4     346.0     349.4
## adjCV     353.3       353     344.6     344.3     343.7     346.9
## 
## TRAINING: % variance explained
##         1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
## X         38.31    60.16    70.84    79.03    84.29    88.63    92.26
## Salary    40.63    41.58    42.17    43.22    44.90    46.48    46.69
##         8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
## X         94.96    96.28     97.26     97.98     98.65     99.15     99.47
## Salary    46.75    46.86     47.76     47.82     47.85     48.10     50.40
##         15 comps  16 comps  17 comps  18 comps  19 comps
## X          99.75     99.89     99.97     99.99    100.00
## Salary     50.55     53.01     53.85     54.61     54.61</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">validationplot</span>(pcr.fit, <span class="dt">val.type =</span> <span class="st">&quot;MSEP&quot;</span>)</code></pre></div>
<p><img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-97-1.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="cross-validation-and-bootstrapping.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="nonlinearity-in-linear-models.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dtkaplan/math253/edit/master/610-Regularization-Shrinkage.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
