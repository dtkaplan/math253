<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Notes for Math 253: Statistical Computing and Machine Learning</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Notes and other materials for Math 253 at Macalester College.">
  <meta name="generator" content="bookdown 0.1.15 and GitBook 2.6.7">

  <meta property="og:title" content="Notes for Math 253: Statistical Computing and Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and other materials for Math 253 at Macalester College." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Notes for Math 253: Statistical Computing and Machine Learning" />
  
  <meta name="twitter:description" content="Notes and other materials for Math 253 at Macalester College." />
  

<meta name="author" content="Daniel Kaplan">

  

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduction.html">
<link rel="next" href="foundations-linear-algebra-likelihood-and-bayes-rule.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math 253 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#math-253-and-the-macalester-statistics-curriculum"><i class="fa fa-check"></i>Math 253 and the Macalester statistics curriculum</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#these-notes-are-written-in-bookdown"><i class="fa fa-check"></i>These notes are written in Bookdown</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#statistical-and-machine-learning"><i class="fa fa-check"></i><b>1.1</b> Statistical and Machine Learning</a><ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#example-1-machine-translation-of-natural-languages"><i class="fa fa-check"></i><b>1.1.1</b> Example 1: Machine translation of natural languages</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#example-2-from-library-catalogs-to-latent-semantic-indexing"><i class="fa fa-check"></i><b>1.1.2</b> Example 2: From library catalogs to latent semantic indexing</a></li>
<li class="chapter" data-level="1.1.3" data-path="introduction.html"><a href="introduction.html#computing-technique"><i class="fa fa-check"></i><b>1.1.3</b> Computing technique</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#review-of-day-1"><i class="fa fa-check"></i><b>1.2</b> Review of Day 1</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#theoretical-concepts-isl-2.1"><i class="fa fa-check"></i><b>1.3</b> Theoretical concepts ISL §2.1</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#statistics-concepts"><i class="fa fa-check"></i><b>1.3.1</b> Statistics concepts</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#computing-concepts"><i class="fa fa-check"></i><b>1.3.2</b> Computing concepts</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#cross-fertilization"><i class="fa fa-check"></i><b>1.3.3</b> Cross fertilization</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#many-techniques"><i class="fa fa-check"></i><b>1.4</b> Many techniques</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.4.1</b> Unsupervised learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction.html"><a href="introduction.html#supervised-learning"><i class="fa fa-check"></i><b>1.4.2</b> Supervised learning:</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#basic-dicotomies-in-machine-learning"><i class="fa fa-check"></i><b>1.5</b> Basic dicotomies in machine learning</a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#purposes-for-learning"><i class="fa fa-check"></i><b>1.5.1</b> Purposes for learning:</a></li>
<li class="chapter" data-level="1.5.2" data-path="introduction.html"><a href="introduction.html#dicotomies"><i class="fa fa-check"></i><b>1.5.2</b> Dicotomies</a></li>
<li class="chapter" data-level="1.5.3" data-path="introduction.html"><a href="introduction.html#prediction-versus-mechanism"><i class="fa fa-check"></i><b>1.5.3</b> Prediction versus mechanism</a></li>
<li class="chapter" data-level="1.5.4" data-path="introduction.html"><a href="introduction.html#flexibility-versus-variance"><i class="fa fa-check"></i><b>1.5.4</b> Flexibility versus variance</a></li>
<li class="chapter" data-level="1.5.5" data-path="introduction.html"><a href="introduction.html#black-box-vs-interpretable-models"><i class="fa fa-check"></i><b>1.5.5</b> Black box vs interpretable models</a></li>
<li class="chapter" data-level="1.5.6" data-path="introduction.html"><a href="introduction.html#reducible-versus-irreducible-error"><i class="fa fa-check"></i><b>1.5.6</b> Reducible versus irreducible error</a></li>
<li class="chapter" data-level="1.5.7" data-path="introduction.html"><a href="introduction.html#regression-versus-classification"><i class="fa fa-check"></i><b>1.5.7</b> Regression versus classification</a></li>
<li class="chapter" data-level="1.5.8" data-path="introduction.html"><a href="introduction.html#supervised-versus-unsupervised"><i class="fa fa-check"></i><b>1.5.8</b> Supervised versus unsupervised</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#programming-activity-1"><i class="fa fa-check"></i><b>1.6</b> Programming Activity 1</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#review-of-day-2"><i class="fa fa-check"></i><b>1.7</b> Review of Day 2</a><ul>
<li class="chapter" data-level="1.7.1" data-path="introduction.html"><a href="introduction.html#trade-offsdicotomies"><i class="fa fa-check"></i><b>1.7.1</b> Trade-offs/Dicotomies</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#a-classifier-example"><i class="fa fa-check"></i><b>1.8</b> A Classifier example</a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#programming-activity-2"><i class="fa fa-check"></i><b>1.9</b> Programming Activity 2</a></li>
<li class="chapter" data-level="1.10" data-path="introduction.html"><a href="introduction.html#day-3-theory-accuracy-precision-and-bias"><i class="fa fa-check"></i><b>1.10</b> Day 3 theory: accuracy, precision, and bias</a><ul>
<li class="chapter" data-level="1.10.1" data-path="introduction.html"><a href="introduction.html#figure-2.10"><i class="fa fa-check"></i><b>1.10.1</b> Figure 2.10</a></li>
<li class="chapter" data-level="1.10.2" data-path="introduction.html"><a href="introduction.html#another-example-a-smoother-simulated-fx."><i class="fa fa-check"></i><b>1.10.2</b> Another example: A smoother simulated <span class="math inline">\(f(x)\)</span>.</a></li>
<li class="chapter" data-level="1.10.3" data-path="introduction.html"><a href="introduction.html#whats-the-best-of-these-models"><i class="fa fa-check"></i><b>1.10.3</b> What’s the “best” of these models?</a></li>
<li class="chapter" data-level="1.10.4" data-path="introduction.html"><a href="introduction.html#why-is-testing-mse-u-shaped"><i class="fa fa-check"></i><b>1.10.4</b> Why is testing MSE U-shaped?</a></li>
<li class="chapter" data-level="1.10.5" data-path="introduction.html"><a href="introduction.html#measuring-the-variance-of-independent-sources-of-variation"><i class="fa fa-check"></i><b>1.10.5</b> Measuring the variance of independent sources of variation</a></li>
<li class="chapter" data-level="1.10.6" data-path="introduction.html"><a href="introduction.html#equation-2.7"><i class="fa fa-check"></i><b>1.10.6</b> Equation 2.7</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="introduction.html"><a href="introduction.html#programming-basics-i-names-classes-and-objects-progbasics1"><i class="fa fa-check"></i><b>1.11</b> Programming Basics I: Names, classes, and objects {progbasics1}</a><ul>
<li class="chapter" data-level="1.11.1" data-path="introduction.html"><a href="introduction.html#names"><i class="fa fa-check"></i><b>1.11.1</b> Names</a></li>
<li class="chapter" data-level="1.11.2" data-path="introduction.html"><a href="introduction.html#objects"><i class="fa fa-check"></i><b>1.11.2</b> Objects</a></li>
<li class="chapter" data-level="1.11.3" data-path="introduction.html"><a href="introduction.html#vectors"><i class="fa fa-check"></i><b>1.11.3</b> Vectors</a></li>
<li class="chapter" data-level="1.11.4" data-path="introduction.html"><a href="introduction.html#matrices"><i class="fa fa-check"></i><b>1.11.4</b> Matrices</a></li>
<li class="chapter" data-level="1.11.5" data-path="introduction.html"><a href="introduction.html#lists"><i class="fa fa-check"></i><b>1.11.5</b> Lists</a></li>
<li class="chapter" data-level="1.11.6" data-path="introduction.html"><a href="introduction.html#functions"><i class="fa fa-check"></i><b>1.11.6</b> Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="introduction.html"><a href="introduction.html#programming-activity-3"><i class="fa fa-check"></i><b>1.12</b> Programming Activity 3</a></li>
<li class="chapter" data-level="1.13" data-path="introduction.html"><a href="introduction.html#review-of-day-3"><i class="fa fa-check"></i><b>1.13</b> Review of Day 3</a></li>
<li class="chapter" data-level="1.14" data-path="introduction.html"><a href="introduction.html#start-thursday-15-sept."><i class="fa fa-check"></i><b>1.14</b> Start Thursday 15 Sept.</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>2</b> (PART) Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression.html"><a href="linear-regression.html#day-4-preview"><i class="fa fa-check"></i><b>2.1</b> Day 4 Preview</a><ul>
<li class="chapter" data-level="2.1.1" data-path="linear-regression.html"><a href="linear-regression.html#isl-books-statement-on-why-to-study-linear-regression"><i class="fa fa-check"></i><b>2.1.1</b> ISL book’s statement on why to study linear regression</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="linear-regression.html"><a href="linear-regression.html#small-data"><i class="fa fa-check"></i><b>2.2</b> Small data</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression.html"><a href="linear-regression.html#programming-basics-linear-models"><i class="fa fa-check"></i><b>2.3</b> Programming basics: Linear Models</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression.html"><a href="linear-regression.html#review-of-day-4-sept-15-2016"><i class="fa fa-check"></i><b>2.4</b> Review of Day 4, Sept 15, 2016</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression.html"><a href="linear-regression.html#regression-and-interpretability"><i class="fa fa-check"></i><b>2.5</b> Regression and Interpretability</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression.html"><a href="linear-regression.html#toward-an-automated-regression-process"><i class="fa fa-check"></i><b>2.6</b> Toward an automated regression process</a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression.html"><a href="linear-regression.html#selecting-model-terms"><i class="fa fa-check"></i><b>2.7</b> Selecting model terms</a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression.html"><a href="linear-regression.html#programming-basics-graphics"><i class="fa fa-check"></i><b>2.8</b> Programming basics: Graphics</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression.html"><a href="linear-regression.html#in-class-programming-activity"><i class="fa fa-check"></i><b>2.9</b> In-class programming activity</a></li>
<li class="chapter" data-level="2.10" data-path="linear-regression.html"><a href="linear-regression.html#day-5-summary"><i class="fa fa-check"></i><b>2.10</b> Day 5 Summary</a><ul>
<li class="chapter" data-level="2.10.1" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression"><i class="fa fa-check"></i><b>2.10.1</b> Linear regression</a></li>
<li class="chapter" data-level="2.10.2" data-path="linear-regression.html"><a href="linear-regression.html#coefficients-as-quantities"><i class="fa fa-check"></i><b>2.10.2</b> Coefficients as quantities</a></li>
<li class="chapter" data-level="2.10.3" data-path="linear-regression.html"><a href="linear-regression.html#graphics-basics"><i class="fa fa-check"></i><b>2.10.3</b> Graphics basics</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="linear-regression.html"><a href="linear-regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>2.11</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="2.12" data-path="linear-regression.html"><a href="linear-regression.html#in-class-programming-activity-1"><i class="fa fa-check"></i><b>2.12</b> In-class programming activity</a></li>
<li class="chapter" data-level="2.13" data-path="linear-regression.html"><a href="linear-regression.html#day-6-summary"><i class="fa fa-check"></i><b>2.13</b> Day 6 Summary</a></li>
<li class="chapter" data-level="2.14" data-path="linear-regression.html"><a href="linear-regression.html#measuring-accuracy-of-the-model"><i class="fa fa-check"></i><b>2.14</b> Measuring Accuracy of the Model</a></li>
<li class="chapter" data-level="2.15" data-path="linear-regression.html"><a href="linear-regression.html#bias-of-the-model"><i class="fa fa-check"></i><b>2.15</b> Bias of the model</a><ul>
<li class="chapter" data-level="2.15.1" data-path="linear-regression.html"><a href="linear-regression.html#theory-of-whole-model-anova."><i class="fa fa-check"></i><b>2.15.1</b> Theory of whole-model ANOVA.</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="linear-regression.html"><a href="linear-regression.html#forward-backward-and-mixed-selection"><i class="fa fa-check"></i><b>2.16</b> Forward, backward and mixed selection</a></li>
<li class="chapter" data-level="2.17" data-path="linear-regression.html"><a href="linear-regression.html#programming-basics-functions"><i class="fa fa-check"></i><b>2.17</b> Programming Basics: Functions</a></li>
<li class="chapter" data-level="2.18" data-path="linear-regression.html"><a href="linear-regression.html#in-class-programming-activity-2"><i class="fa fa-check"></i><b>2.18</b> In-class programming activity</a></li>
<li class="chapter" data-level="2.19" data-path="linear-regression.html"><a href="linear-regression.html#review-of-day-7"><i class="fa fa-check"></i><b>2.19</b> Review of Day 7</a></li>
<li class="chapter" data-level="2.20" data-path="linear-regression.html"><a href="linear-regression.html#using-predict-to-calculate-precision"><i class="fa fa-check"></i><b>2.20</b> Using predict() to calculate precision</a></li>
<li class="chapter" data-level="2.21" data-path="linear-regression.html"><a href="linear-regression.html#conclusion"><i class="fa fa-check"></i><b>2.21</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html"><i class="fa fa-check"></i><b>3</b> Foundations: linear algebra, likelihood and Bayes’ rule</a><ul>
<li class="chapter" data-level="3.1" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#linear-algebra"><i class="fa fa-check"></i><b>3.1</b> Linear Algebra</a></li>
<li class="chapter" data-level="3.2" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#arithmetic-of-linear-algebra-operations"><i class="fa fa-check"></i><b>3.2</b> Arithmetic of linear algebra operations</a></li>
<li class="chapter" data-level="3.3" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#the-geometry-of-fitting"><i class="fa fa-check"></i><b>3.3</b> The geometry of fitting</a></li>
<li class="chapter" data-level="3.4" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#precision-of-the-coefficients"><i class="fa fa-check"></i><b>3.4</b> Precision of the coefficients</a></li>
<li class="chapter" data-level="3.5" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#likelihood-and-bayes"><i class="fa fa-check"></i><b>3.5</b> Likelihood and Bayes</a></li>
<li class="chapter" data-level="3.6" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#summary-of-day-8"><i class="fa fa-check"></i><b>3.6</b> Summary of Day 8</a></li>
<li class="chapter" data-level="3.7" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#day-9-announcements"><i class="fa fa-check"></i><b>3.7</b> Day 9 Announcements</a><ul>
<li class="chapter" data-level="3.7.1" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#whats-a-probability"><i class="fa fa-check"></i><b>3.7.1</b> What’s a probability?</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#conditional-probability"><i class="fa fa-check"></i><b>3.8</b> Conditional probability</a></li>
<li class="chapter" data-level="3.9" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#inverting-conditional-probabilities"><i class="fa fa-check"></i><b>3.9</b> Inverting conditional probabilities</a></li>
<li class="chapter" data-level="3.10" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#summary-of-day-9"><i class="fa fa-check"></i><b>3.10</b> Summary of Day 9</a></li>
<li class="chapter" data-level="3.11" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#likelihood-example"><i class="fa fa-check"></i><b>3.11</b> Likelihood example</a></li>
<li class="chapter" data-level="3.12" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#exponential-probability-density"><i class="fa fa-check"></i><b>3.12</b> Exponential probability density</a><ul>
<li class="chapter" data-level="3.12.1" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#meanwhile-further-north"><i class="fa fa-check"></i><b>3.12.1</b> Meanwhile, further north …</a></li>
</ul></li>
<li class="chapter" data-level="3.13" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#california-earthquake-warning-reprise"><i class="fa fa-check"></i><b>3.13</b> California earthquake warning, reprise</a></li>
<li class="chapter" data-level="3.14" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#the-price-is-right"><i class="fa fa-check"></i><b>3.14</b> The Price is Right!</a></li>
<li class="chapter" data-level="3.15" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#from-likelihood-to-bayes"><i class="fa fa-check"></i><b>3.15</b> From likelihood to Bayes</a></li>
<li class="chapter" data-level="3.16" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#choosing-models-using-maximum-likelihood"><i class="fa fa-check"></i><b>3.16</b> Choosing models using maximum likelihood</a></li>
<li class="chapter" data-level="3.17" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#day-9-review"><i class="fa fa-check"></i><b>3.17</b> Day 9 Review</a></li>
<li class="chapter" data-level="3.18" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#reading-what-is-bayesian-statistics"><i class="fa fa-check"></i><b>3.18</b> Reading: <em>What is Bayesian Statistics</em></a></li>
<li class="chapter" data-level="3.19" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#programming-basics-conditionals"><i class="fa fa-check"></i><b>3.19</b> Programming Basics: Conditionals</a></li>
<li class="chapter" data-level="3.20" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#ifelse-examples"><i class="fa fa-check"></i><b>3.20</b> <code>ifelse()</code> examples</a></li>
<li class="chapter" data-level="3.21" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#if-else-examples"><i class="fa fa-check"></i><b>3.21</b> if … else … examples</a></li>
<li class="chapter" data-level="3.22" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#simple"><i class="fa fa-check"></i><b>3.22</b> Simple</a></li>
<li class="chapter" data-level="3.23" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#blood-testing"><i class="fa fa-check"></i><b>3.23</b> Blood testing</a></li>
<li class="chapter" data-level="3.24" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#the-hyper-volume-of-the-hypersphere."><i class="fa fa-check"></i><b>3.24</b> The (hyper)-volume of the hypersphere.</a></li>
<li class="chapter" data-level="3.25" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#find-the-surface-area-d_n-rn-1."><i class="fa fa-check"></i><b>3.25</b> Find the surface area, <span class="math inline">\(D_n r^{n-1}\)</span>.</a></li>
<li class="chapter" data-level="3.26" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#in-class-programming-activity-3"><i class="fa fa-check"></i><b>3.26</b> In-class programming activity</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classifiers.html"><a href="classifiers.html"><i class="fa fa-check"></i><b>4</b> Classifiers</a><ul>
<li class="chapter" data-level="4.1" data-path="classifiers.html"><a href="classifiers.html#classification-overview"><i class="fa fa-check"></i><b>4.1</b> Classification overview</a></li>
<li class="chapter" data-level="4.2" data-path="classifiers.html"><a href="classifiers.html#day-10-preview"><i class="fa fa-check"></i><b>4.2</b> Day 10 preview</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5</b> Logistic Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#probability-and-odds"><i class="fa fa-check"></i><b>5.1</b> Probability and odds</a></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression.html"><a href="logistic-regression.html#log-odds"><i class="fa fa-check"></i><b>5.2</b> Log Odds</a></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression.html"><a href="logistic-regression.html#why-use-odds"><i class="fa fa-check"></i><b>5.3</b> Why use odds?</a></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression.html"><a href="logistic-regression.html#use-of-glm"><i class="fa fa-check"></i><b>5.4</b> Use of glm()</a></li>
<li class="chapter" data-level="5.5" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>5.5</b> Interpretation of coefficients</a></li>
<li class="chapter" data-level="5.6" data-path="logistic-regression.html"><a href="logistic-regression.html#example-logistic-regression-of-default"><i class="fa fa-check"></i><b>5.6</b> Example: Logistic regression of default</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html"><i class="fa fa-check"></i><b>6</b> Linear and Quadratic Discriminant Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#example-default-on-student-loans"><i class="fa fa-check"></i><b>6.1</b> Example: Default on student loans</a></li>
<li class="chapter" data-level="6.2" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#a-bayes-rule-approach"><i class="fa fa-check"></i><b>6.2</b> A Bayes’ Rule approach</a></li>
<li class="chapter" data-level="6.3" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#univariate-gaussian"><i class="fa fa-check"></i><b>6.3</b> Univariate Gaussian</a></li>
<li class="chapter" data-level="6.4" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#uncorrelated-bivariate-gaussian"><i class="fa fa-check"></i><b>6.4</b> Uncorrelated bivariate gaussian</a></li>
<li class="chapter" data-level="6.5" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#bivariate-normal-distribution-with-correlations"><i class="fa fa-check"></i><b>6.5</b> Bivariate normal distribution with correlations</a></li>
<li class="chapter" data-level="6.6" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#shape-of-multivariate-gaussian"><i class="fa fa-check"></i><b>6.6</b> Shape of multivariate gaussian</a></li>
<li class="chapter" data-level="6.7" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#generating-bivariate-normal-from-independent"><i class="fa fa-check"></i><b>6.7</b> Generating bivariate normal from independent</a></li>
<li class="chapter" data-level="6.8" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#independent-variables-x_i"><i class="fa fa-check"></i><b>6.8</b> Independent variables <span class="math inline">\(x_i\)</span></a></li>
<li class="chapter" data-level="6.9" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#re-explaining-boldsymbolsigma"><i class="fa fa-check"></i><b>6.9</b> Re-explaining <span class="math inline">\(\boldsymbol\Sigma\)</span></a></li>
<li class="chapter" data-level="6.10" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#lda"><i class="fa fa-check"></i><b>6.10</b> LDA</a></li>
<li class="chapter" data-level="6.11" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#qda"><i class="fa fa-check"></i><b>6.11</b> QDA</a></li>
<li class="chapter" data-level="6.12" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#error-test-rates-on-various-classifiers"><i class="fa fa-check"></i><b>6.12</b> Error test rates on various classifiers</a></li>
<li class="chapter" data-level="6.13" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#error-rates"><i class="fa fa-check"></i><b>6.13</b> Error rates</a></li>
<li class="chapter" data-level="6.14" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#receiver-operating-curves"><i class="fa fa-check"></i><b>6.14</b> Receiver operating curves</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html"><i class="fa fa-check"></i><b>7</b> Cross-Validation and Bootstrapping</a><ul>
<li class="chapter" data-level="7.1" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#philosophical-approaches"><i class="fa fa-check"></i><b>7.1</b> Philosophical approaches</a><ul>
<li class="chapter" data-level="7.1.1" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#occams-razor-a-heuristic"><i class="fa fa-check"></i><b>7.1.1</b> Occam’s Razor: A heuristic</a></li>
<li class="chapter" data-level="7.1.2" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#einsteins-proverb"><i class="fa fa-check"></i><b>7.1.2</b> Einstein’s proverb</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#operationalizing-model-choice"><i class="fa fa-check"></i><b>7.2</b> Operationalizing model choice</a></li>
<li class="chapter" data-level="7.3" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#some-definitions-of-better"><i class="fa fa-check"></i><b>7.3</b> Some definitions of “better”</a></li>
<li class="chapter" data-level="7.4" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#training-and-testing"><i class="fa fa-check"></i><b>7.4</b> Training and Testing</a></li>
<li class="chapter" data-level="7.5" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#trade-off"><i class="fa fa-check"></i><b>7.5</b> Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="programming-basics.html"><a href="programming-basics.html"><i class="fa fa-check"></i><b>8</b> Programming Basics</a><ul>
<li class="chapter" data-level="8.1" data-path="programming-basics.html"><a href="programming-basics.html#loopsiteration"><i class="fa fa-check"></i><b>8.1</b> Loops/Iteration</a></li>
<li class="chapter" data-level="8.2" data-path="programming-basics.html"><a href="programming-basics.html#parts-of-a-loop"><i class="fa fa-check"></i><b>8.2</b> Parts of a loop</a></li>
<li class="chapter" data-level="8.3" data-path="programming-basics.html"><a href="programming-basics.html#trivial-examples"><i class="fa fa-check"></i><b>8.3</b> Trivial examples</a></li>
<li class="chapter" data-level="8.4" data-path="programming-basics.html"><a href="programming-basics.html#bootstrapping"><i class="fa fa-check"></i><b>8.4</b> Bootstrapping</a></li>
<li class="chapter" data-level="8.5" data-path="programming-basics.html"><a href="programming-basics.html#leave-one-out-cross-validation."><i class="fa fa-check"></i><b>8.5</b> Leave-one-out cross-validation.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i>Appendices</a></li>
<li class="chapter" data-level="" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html"><i class="fa fa-check"></i>Connecting RStudio to your GitHub repository</a><ul>
<li class="chapter" data-level="8.6" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html#setting-up-your-math-253-repository"><i class="fa fa-check"></i><b>8.6</b> Setting up your Math 253 repository</a></li>
<li class="chapter" data-level="8.7" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html#using-your-repository"><i class="fa fa-check"></i><b>8.7</b> Using your repository</a></li>
<li class="chapter" data-level="8.8" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html#why-are-we-doing-this"><i class="fa fa-check"></i><b>8.8</b> Why are we doing this?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="instructions-for-the-publishing-system-bookdown.html"><a href="instructions-for-the-publishing-system-bookdown.html"><i class="fa fa-check"></i>Instructions for the publishing system: Bookdown</a></li>
<li class="divider"></li>
<li><a href="https://github.com/dtkaplan/math253" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for Math 253: Statistical Computing and Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1">
<h1><span class="header-section-number">Topic 2</span> (PART) Linear Regression</h1>
<div id="day-4-preview" class="section level2">
<h2><span class="header-section-number">2.1</span> Day 4 Preview</h2>
<ul>
<li>The linear model (e.g. what <code>lm()</code> does)</li>
<li>A variety of questions relevant to different purposes, e.g.
<ul>
<li>how good will a prediction be?</li>
<li>what’s the strength of an effect?</li>
<li>is there synergy between different factors?</li>
</ul></li>
</ul>
<div id="isl-books-statement-on-why-to-study-linear-regression" class="section level3">
<h3><span class="header-section-number">2.1.1</span> ISL book’s statement on why to study linear regression</h3>
<blockquote>
<p>“Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described … later …, linear regression is still a useful and widely used statistical learning method. Moreover, it serves as a good jumping-off point for newer approaches…. Consequently, the importance of having a good understanding of linear regression before studying more complex learning methods cannot be overstated.”</p>
</blockquote>
<p>Concepts from linear regression:</p>
<ul>
<li>Compact representation of model form: polynomial coefficients.</li>
<li>Much of inference (confidence intervals, hypothesis tests) can be expressed in terms of a polynomial coefficient.</li>
<li>“Size” of model quantifiable as an integer: number of coefficients: degrees of freedom.</li>
<li>Highly efficient estimation (when doing least squares)</li>
</ul>
</div>
</div>
<div id="small-data" class="section level2">
<h2><span class="header-section-number">2.2</span> Small data</h2>
<p>The regression techniques were developed in an era of small data, such as that that might be written in a lab notebook or field journal. As a result:</p>
<ol style="list-style-type: decimal">
<li>Emphasis on very simple descriptions, such as means, differences between means, simple regression.</li>
<li>Theoretical concern with details of distributions, such as the t-distribution.
<ul>
<li>the difference between z- and t-distributions are of no consequence for moderate DF and higher.</li>
</ul></li>
<li>No division into training and testing data. Data are too valuable to test! (Ironic, given the importance of replicability in the theory of the scientific method.)</li>
</ol>
<p>As a consequence of (3), there’s a great deal of concern about <em>assumptions</em>, e.g.</p>
<ul>
<li>linearity of <span class="math inline">\(f({\mathbf X})\)</span></li>
<li>structure of <span class="math inline">\(\epsilon\)</span>: IID — Independent and Identically Distributed
<ul>
<li>uncorrelated between cases</li>
<li>each is a draw from the same distribution.</li>
</ul></li>
</ul>
</div>
<div id="programming-basics-linear-models" class="section level2">
<h2><span class="header-section-number">2.3</span> Programming basics: Linear Models</h2>
<p>Syntactic element: <strong>formulas</strong>. Formulas provide a way of using variables “symbolically.” This is useful, for instance, in depicting the desired relationship among variables. Two forms:</p>
<ul>
<li><code>y ~ x</code> two-sided</li>
<li><code>~ x</code> one-sided</li>
<li>NOT ALLOWED, <code>y ~</code></li>
</ul>
<p>Important functions:</p>
<ul>
<li><code>lm()</code>, <code>predict()</code> , <code>anova()</code>, <code>summary()</code>.</li>
<li>For later: <code>solve()</code>, <code>model.matrix()</code>.</li>
<li>From 155: <code>coef()</code>, <code>fitted()</code>, <code>resid()</code></li>
</ul>
<p>Training with <code>lm()</code>: Specify formula <code>Y ~ X1 + X2</code> …</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(College, <span class="dt">package =</span> <span class="st">&quot;ISLR&quot;</span>)
mod &lt;-<span class="st"> </span><span class="kw">lm</span>(Outstate ~<span class="st"> </span>Enroll +<span class="st"> </span>Accept +<span class="st"> </span>perc.alumni, <span class="dt">data =</span> College ) 
<span class="kw">coef</span>(mod)</code></pre></div>
<pre><code>## (Intercept)      Enroll      Accept perc.alumni 
## 6387.368606   -2.888077    1.101019  179.528731</code></pre>
<p>What kind of thing is <code>mod</code>?</p>
<p>Model output with <code>predict()</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod, 
        <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">Enroll =</span> <span class="dv">100</span>, <span class="dt">Accept =</span> <span class="dv">1000</span>, <span class="dt">perc.alumni =</span> <span class="dv">25</span>))</code></pre></div>
<pre><code>##       1 
## 11687.8</code></pre>
<p>What kind of thing is the output of <code>predict()</code>?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod, <span class="dt">interval =</span> <span class="st">&quot;confidence&quot;</span>,
        <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">Enroll =</span> <span class="dv">100</span>, <span class="dt">Accept =</span> <span class="dv">1000</span>, <span class="dt">perc.alumni =</span> <span class="dv">25</span>))</code></pre></div>
<pre><code>##       fit      lwr      upr
## 1 11687.8 11383.57 11992.03</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(mod, <span class="dt">interval =</span> <span class="st">&quot;prediction&quot;</span>,
        <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">Enroll =</span> <span class="dv">100</span>, <span class="dt">Accept =</span> <span class="dv">1000</span>, <span class="dt">perc.alumni =</span> <span class="dv">25</span>))</code></pre></div>
<pre><code>##       fit      lwr      upr
## 1 11687.8 5548.956 17826.64</code></pre>
<p>Why is the “confidence interval” so much narrower than the “prediction interval?”</p>
<p><strong>Inference</strong> with <code>anova()</code> and <code>summary()</code>. This is all “in-sample” inference, not cross-validated.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">M &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(~<span class="st"> </span>Enroll +<span class="st"> </span>Accept *<span class="st"> </span>perc.alumni, <span class="dt">data =</span> College)
<span class="kw">qr.solve</span>(M, College$Outstate)</code></pre></div>
<pre><code>##        (Intercept)             Enroll             Accept 
##      7024.34843865        -3.00377409         0.78401053 
##        perc.alumni Accept:perc.alumni 
##       147.31268325         0.02011475</code></pre>
<p>For the people who have had linear algebra, why doesn’t this work?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">solve</span>(M, College$Outstate)</code></pre></div>
<pre><code>## Error in solve.default(M, College$Outstate): &#39;a&#39; (777 x 5) must be square</code></pre>
<p><a href="../ProgrammingActivities/Day-03-Programming.html">Indexing on data: training and testing data sets</a></p>
</div>
<div id="review-of-day-4-sept-15-2016" class="section level2">
<h2><span class="header-section-number">2.4</span> Review of Day 4, Sept 15, 2016</h2>
<ul>
<li>Discussed the linear regression architecture and how it relates to machine learning.
<ul>
<li>linear regression is designed to work even in “small data” situations where
<ol style="list-style-type: lower-alpha">
<li>cross-validation is not appropriate</li>
<li>the number of model degrees of freedom may be almost as large as <span class="math inline">\(n\)</span></li>
</ol></li>
<li>provides a ready definition of the “size” of a model: the number of coefficients.</li>
</ul></li>
<li>Introduced the main software used in linear regression, <code>lm()</code> and <code>predict()</code></li>
<li>Programming basics: indexing of vectors, matrices and data frames.</li>
</ul>
</div>
<div id="regression-and-interpretability" class="section level2">
<h2><span class="header-section-number">2.5</span> Regression and Interpretability</h2>
<p>Regression models are generally constructed for the sake of interpretability:</p>
<ul>
<li>Global linearity</li>
<li>Coefficients are indication of effect size. The coefficients have physical units.</li>
<li>Term by term indication of statistical significance</li>
</ul>
<p>An example on <code>College</code> data from <code>ISLR</code> package</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(College, <span class="dt">package=</span><span class="st">&quot;ISLR&quot;</span>)
College$Yield &lt;-<span class="st"> </span><span class="kw">with</span>(College, Enroll/Accept)
mod1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Yield ~<span class="st"> </span>Outstate +<span class="st"> </span>Grad.Rate +<span class="st"> </span>Top25perc, <span class="dt">data =</span> College)
mosaic::<span class="kw">rsquared</span>(mod1)</code></pre></div>
<pre><code>## [1] 0.2170221</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Yield ~<span class="st"> </span>. -<span class="st"> </span>Grad.Rate, <span class="dt">data =</span> College)
mosaic::<span class="kw">rsquared</span>(mod2)</code></pre></div>
<pre><code>## [1] 0.5004599</code></pre>
<ul>
<li>What variables matter?</li>
<li>How good are the predictions?</li>
<li>How strong are the effects?</li>
</ul>
</div>
<div id="toward-an-automated-regression-process" class="section level2">
<h2><span class="header-section-number">2.6</span> Toward an automated regression process</h2>
<p>In machine learning, we ask the computer to identify patterns in the data.</p>
<ul>
<li>In “traditional” regression (which is still very important), we specify the explanatory terms and the computer finds the “best” model with those terms: least squares.</li>
<li>In machine learning, we want the computer to figure out which terms, of all the possibilities, will lead to the “best” model.</li>
</ul>
</div>
<div id="selecting-model-terms" class="section level2">
<h2><span class="header-section-number">2.7</span> Selecting model terms</h2>
<p>The regression techniques</p>
<ul>
<li>Traditional regression:
<ul>
<li>Our knowledge of the system being studied.</li>
<li>Heirarchical principal
<ul>
<li>main effects, then</li>
<li>interaction.</li>
</ul></li>
</ul></li>
<li>Machine learning
<ul>
<li>Look at all combinations of variables?</li>
<li>Activity 1:
<ul>
<li>Write a statement that will pull 2 random variables from a data frame <em>and</em> the explanatory variable.</li>
<li>Use <code>Yield ~ .</code> as the formula.</li>
</ul></li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">explanatory_vars &lt;-<span class="st"> </span><span class="kw">names</span>(College)[-<span class="dv">19</span>]

my_vars &lt;-<span class="st"> </span><span class="kw">sample</span>(explanatory_vars, <span class="dt">size =</span> <span class="dv">12</span>)
new_data &lt;-<span class="st"> </span>College[ , <span class="kw">c</span>(my_vars, <span class="st">&quot;Yield&quot;</span>)]
mod &lt;-<span class="st"> </span><span class="kw">lm</span>(Yield ~<span class="st"> </span>., <span class="dt">data =</span> new_data)
mosaic::<span class="kw">rsquared</span>(mod)</code></pre></div>
<pre><code>## [1] 0.3339102</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Yield ~ ., data = new_data)
## 
## Coefficients:
## (Intercept)  P.Undergrad    Top10perc     Terminal   Room.Board  
##   6.205e-01    7.134e-06    2.840e-03   -1.454e-03   -3.223e-05  
##      Expend  perc.alumni       Enroll    Grad.Rate         Apps  
##   1.555e-06   -4.990e-04    7.972e-05   -5.487e-04   -2.213e-05  
##   S.F.Ratio    Top25perc     Personal  
##   3.514e-03   -1.115e-03    9.487e-06</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span>^<span class="dv">18</span></code></pre></div>
<pre><code>## [1] 262144</code></pre>
<ul>
<li>Activity 2: * How many combinations are there of <span class="math inline">\(k\)</span> explanatory variables? Calculate this for <span class="math inline">\(k = 5, 10, 15, 20\)</span>. How many are there in the <code>College</code>? * What about with interactions?</li>
<li>How long does it take to fit a model?</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system.time</span>( <span class="kw">do</span>(<span class="dv">1000</span>) *<span class="st"> </span><span class="kw">lm</span>(Yield ~<span class="st"> </span>., <span class="dt">data=</span>College))</code></pre></div>
<pre><code>##    user  system elapsed 
##   5.937   0.024   5.967</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">256</span>*<span class="dv">7</span>/<span class="dv">3600</span></code></pre></div>
<pre><code>## [1] 0.4977778</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(College)</code></pre></div>
<pre><code>##  [1] &quot;Private&quot;     &quot;Apps&quot;        &quot;Accept&quot;      &quot;Enroll&quot;      &quot;Top10perc&quot;  
##  [6] &quot;Top25perc&quot;   &quot;F.Undergrad&quot; &quot;P.Undergrad&quot; &quot;Outstate&quot;    &quot;Room.Board&quot; 
## [11] &quot;Books&quot;       &quot;Personal&quot;    &quot;PhD&quot;         &quot;Terminal&quot;    &quot;S.F.Ratio&quot;  
## [16] &quot;perc.alumni&quot; &quot;Expend&quot;      &quot;Grad.Rate&quot;   &quot;Yield&quot;</code></pre>
<p>With <span class="math inline">\(k\)</span> explanatory variables, <span class="math inline">\(2^k\)</span> possibilities, not even including interactions. Including first-order interactions, it’s <span class="math inline">\(2^k + 2^{k(k-1)/2}\)</span>. Calculate this for $k=3. - Increase in <span class="math inline">\(R^2\)</span>? Problem: <span class="math inline">\(R^2\)</span> will always go up as we add a new term. - Some other measure that takes into account how much <span class="math inline">\(R^2\)</span> should go up.</p>
</div>
<div id="programming-basics-graphics" class="section level2">
<h2><span class="header-section-number">2.8</span> Programming basics: Graphics</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="dv">1</span>, <span class="dt">type =</span> <span class="st">&quot;n&quot;</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">200</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">300</span>,<span class="dv">500</span>))</code></pre></div>
<p><img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Basic functions:</p>
<ol style="list-style-type: decimal">
<li>Create a frame: <code>plot()</code>. Blank frame: <code>plot( , type=&quot;n&quot;)</code>
<ul>
<li>set axis limits,</li>
</ul></li>
<li>Dots: <code>points(x, y)</code>, <code>pch=20</code></li>
<li>Lines: <code>lines(x, y)</code> — with <code>NA</code> for line breaks</li>
<li>Polygons: <code>polygon(x, y)</code> — like lines but connects first to last.
<ul>
<li>fill</li>
</ul></li>
<li>Color, size, … <code>rgb(r, g, b, alpha)</code>, “tomato”</li>
</ol>
</div>
<div id="in-class-programming-activity" class="section level2">
<h2><span class="header-section-number">2.9</span> In-class programming activity</h2>
<p><a href="../Daily-Programming/Day-4-Programming-Task.pdf">Programming Activity 4</a>: Drawing a histogram.</p>
</div>
<div id="day-5-summary" class="section level2">
<h2><span class="header-section-number">2.10</span> Day 5 Summary</h2>
<div id="linear-regression" class="section level3">
<h3><span class="header-section-number">2.10.1</span> Linear regression</h3>
<ul>
<li>Discussed “interpretability” of linear models, e.g. meaning of coefficients, confidence intervals, R^2, etc.
<ul>
<li>which variables are “important” via ANOVA and mean sum of squares</li>
</ul></li>
<li>Discussed metrics to compare models
<ul>
<li>R^2 – not fair, since “bigger” models are always better</li>
<li>Punishment: Two criteria for judging
<ul>
<li>R^2</li>
<li>How big the model is.</li>
<li>These two are somehow combined together into “adjusted R^2.” We’ll say more about that today.</li>
</ul></li>
<li>Cross-validation. Judge each model on its “out of sample” prediction performance.</li>
</ul></li>
</ul>
</div>
<div id="coefficients-as-quantities" class="section level3">
<h3><span class="header-section-number">2.10.2</span> Coefficients as quantities</h3>
<p>Coefficients in linear models are not just numbers, they are physical quantities with dimensions and units.</p>
<ul>
<li>Dimensions are always (dim of response)/(dim of this term)</li>
<li>The model doesn’t depend on the units of these quantities. The units only set the magnitude to the numerical part of the coefficient, but as a quantity a coefficient is the same thing regardless of units.</li>
<li>Conversion from one unit to another by multiplying by 1, but expressed in different units, e.g. 60 seconds per minute, 2.2 pounds per kilogram.</li>
</ul>
</div>
<div id="graphics-basics" class="section level3">
<h3><span class="header-section-number">2.10.3</span> Graphics basics</h3>
<ol style="list-style-type: decimal">
<li>API for graphics: <code>plot()</code>, <code>points()</code>, <code>lines()</code>, <code>polygon()</code>, <code>text()</code>, …</li>
</ol>
</div>
</div>
<div id="k-nearest-neighbors" class="section level2">
<h2><span class="header-section-number">2.11</span> K-nearest neighbors</h2>
<p>K-nearest neighbors is a simple, general kind of function-building method. But some problems:</p>
<ul>
<li>Interpretability: but you can always take partial derivatives.</li>
<li>When you have prediction (aka “explanatory”) variables in dollars and in miles, how do you calculate the distance between points? What are the dimensions of distance?
<ul>
<li>Dimensionality refers to the physical feature, e.g. time, distance, area, volume, money, charge, luminance, mass, …</li>
<li>Units are the ways in which dimensions are measured, e.g., cups, gallons, liters … all refer to volume
<ul>
<li>Give some examples of units for each of the dimensions.</li>
<li>Some everyday quantities are dimensionless, e.g. pure numbers. Give some examples: … (angles, percent, fractions, … but not ratios in general.)</li>
</ul></li>
<li>Regression fixes units automatically, since the coefficients themselves have dimensionality. They will adjust automatically to changes in units, so the model is the same regardless of whether we use miles, km, parsecs, …</li>
<li>In KNN, to avoid dependence on units, need to do some standardization by dividing by something in the same units, e.g. sd.</li>
</ul></li>
<li>Curse of dimensionality. Let’s create 1000 randomly placed points in the unit square:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rpts &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(<span class="dv">2</span>*<span class="dv">1000</span>), <span class="dt">ncol=</span><span class="dv">2</span>)</code></pre></div>
<p>What’s the distribution of distances from a single random point to the 1000 others:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">our_point &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">2</span>)</code></pre></div>
<p>The distance between our point and each of the others</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tmp &lt;-<span class="st"> </span><span class="kw">matrix</span>(our_point, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">nrow=</span><span class="dv">1000</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)
delta &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">rowSums</span>((rpts -<span class="st"> </span>tmp)^<span class="dv">2</span>))</code></pre></div>
<ul>
<li>How far away is a typical point?</li>
<li>Write a function that takes the matrix of points and the “our point” and finds the distance from our point to each and every one of the points in the matrix.</li>
<li>How far away is a typical point in 1-dimensional space?</li>
<li>In 10-dimensional space?</li>
<li>In 100-dimensional space?</li>
</ul>
</div>
<div id="in-class-programming-activity-1" class="section level2">
<h2><span class="header-section-number">2.12</span> In-class programming activity</h2>
<p><a href="../Daily-Programming/Day-05-Programming-Task.pdf">Day 5 activity</a></p>
<p>Drawing a histogram.</p>
</div>
<div id="day-6-summary" class="section level2">
<h2><span class="header-section-number">2.13</span> Day 6 Summary</h2>
<ul>
<li><span class="math inline">\(R^2\)</span>
<ul>
<li>var(fitted) / var(response)</li>
<li>partitioning of variance:
<ul>
<li>var(fitted) + var(resid) = var(response)</li>
<li>same with sum of squares: SS(fitted) + SS(resid) = SS(response)</li>
</ul></li>
</ul></li>
<li>Adjusted <span class="math inline">\(R^2\)</span>
<ul>
<li><span class="math inline">\(R^2\)</span> vs <span class="math inline">\(p\)</span> picture</li>
<li>Derive a formula from the picture: we’ve got <span class="math inline">\(p+1\)</span> df to get from <span class="math inline">\(R^2 = 0\)</span> to our observed <span class="math inline">\(R^2\)</span>, so <span class="math inline">\(n - (p+1)\)</span> df left for the residuals. Rate of increase due to junk is <span class="math inline">\((1 - R^2) / (n - p - 1)\)</span>. Projecting back <span class="math inline">\(p\)</span> terms gives an adjustment of <span class="math inline">\((1 - R^2) / \frac{p}{n - p - 1}\)</span>. Subtract this from <span class="math inline">\(R^2\)</span>.</li>
<li>Wikipedia gives two formulas:
<ul>
<li><span class="math inline">\(Adj R^2 = {1-(1-R^{2}){n-1 \over n-p-1}}\)</span> – this projects back <span class="math inline">\(n-1\)</span> terms from 1.</li>
<li><span class="math inline">\(Adj R^2 = {R^{2}-(1-R^{2}){p \over n-p-1}}\)</span> — projects back <span class="math inline">\(p\)</span> terms from <span class="math inline">\(R^2\)</span>.</li>
</ul></li>
</ul></li>
<li>Adjusted <span class="math inline">\(R^2\)</span></li>
<li>Whole model ANOVA.</li>
<li>ANOVA on model parts</li>
</ul>
</div>
<div id="measuring-accuracy-of-the-model" class="section level2">
<h2><span class="header-section-number">2.14</span> Measuring Accuracy of the Model</h2>
<ul>
<li><span class="math inline">\(R^2\)</span> = Var(fitted)/Var(response)</li>
<li>Adjusted <span class="math inline">\(R^2\)</span> - takes into account estimate of average increase in <span class="math inline">\(R^2\)</span> per junk degree of freedom</li>
<li>Residual Standard Error - Sqrt of average square error per residual degree of freedom. The sqrt of the mean square for residuals in ANOVA.</li>
</ul>
</div>
<div id="bias-of-the-model" class="section level2">
<h2><span class="header-section-number">2.15</span> Bias of the model</h2>
<p>You need to know the “truth” to calculate the bias. We don’t.</p>
<p><img src="Images/Chapter-2/2.1.png" width="400" /></p>
<ul>
<li>Perhaps effect of TV goes as sqrt(money) as media get saturated?</li>
<li>Perhaps there is a synergy that wasn’t included in the model?</li>
</ul>
<div id="theory-of-whole-model-anova." class="section level3">
<h3><span class="header-section-number">2.15.1</span> Theory of whole-model ANOVA.</h3>
<p>Standard measure: <span class="math inline">\(\frac{\mbox{Explained amount}}{\mbox{Unexplained amount}}\)</span></p>
<p>Examples:</p>
<ul>
<li>Standard error of mean: <span class="math inline">\(\frac{\hat{\mu}}{\sigma / n}\)</span> – note the <span class="math inline">\(n\)</span>.</li>
<li>t statistic on difference between two means: <span class="math inline">\(\frac{\hat{\mu}_1 - \hat{\mu}_2}{\sigma / (n-1)}\)</span></li>
<li>F statistic: <span class="math inline">\(\frac{SS / df1}{SSR / df2}\)</span>
<ul>
<li>df1 is the number of degrees of freedom involved by the model or model term under consideration.</li>
<li>df2 is <span class="math inline">\(n - (p - 1)\)</span> where <span class="math inline">\(p\)</span> is the total degrees of freedom in the model. (I called this <span class="math inline">\(m\)</span> in the Math 155 book.) The intercept is what the <span class="math inline">\(-1\)</span> is about: the intercept <em>can never</em> account for case-to-case variation.</li>
</ul></li>
</ul>
<p>Trade-off between eating variance and consuming degrees of freedom.</p>
</div>
</div>
<div id="forward-backward-and-mixed-selection" class="section level2">
<h2><span class="header-section-number">2.16</span> Forward, backward and mixed selection</h2>
<p>Use the <code>College</code> model to demonstrate each of the approaches by hand. Start with <code>pairs()</code> or write an <code>lapply()</code> for the correlation with <code>Yield</code>?</p>
<p>Create a whole bunch of model terms</p>
<ul>
<li>“main” effects</li>
<li>“interaction” effects</li>
<li>nonlinear transformations: powers, logs, sqrt, steps, …</li>
<li>categorical variables</li>
</ul>
<p>Result: a set of <span class="math inline">\(k\)</span> vectors that we’re interested to use in our model.</p>
<p>Considerations:</p>
<ul>
<li>not all of the <span class="math inline">\(k\)</span> vectors may pull their weight</li>
<li>two or more vectors may overlap in how they eat up variance</li>
</ul>
<p>Algorithmic approaches:</p>
<ul>
<li>Try all combinations, pick the best one.
<ul>
<li>computationally expensive/impossible <span class="math inline">\(2^k\)</span> possibilities</li>
<li>what’s the sensitivity of the process to the choice of training data?</li>
</ul></li>
<li>“Greedy” approaches</li>
</ul>
</div>
<div id="programming-basics-functions" class="section level2">
<h2><span class="header-section-number">2.17</span> Programming Basics: Functions</h2>
<ol style="list-style-type: decimal">
<li><p>Syntax of functions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">name &lt;-<span class="st"> </span>function(arg1, arg2, ...) {
  body of function. Can use arg1, arg2, etc. 
}</code></pre></div></li>
</ol>
<ul>
<li>typically you will return a value. The value calculated by the last command line in the body is what’s returned. Or you can use <code>return()</code> at any point in the function.</li>
<li>Often functions are designed to produce “side effects”, e.g. graphics.<br />
</li>
<li>Scope: what happens in functions stays in functions.</li>
</ul>
<ol style="list-style-type: decimal">
<li>Create a plotting frame: <code>plot()</code>
<ul>
<li>Write a function that makes this more convenient to use. What features would you like.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">blank_frame &lt;-<span class="st"> </span>function(xlim, ylim) {

}</code></pre></div></li>
<li>Write a function to draw a circle.
<ul>
<li>What do you want the interface to look like? What arguments are essential? What options are nice to have?</li>
</ul></li>
</ol>
</div>
<div id="in-class-programming-activity-2" class="section level2">
<h2><span class="header-section-number">2.18</span> In-class programming activity</h2>
<p>Histogram and density functions</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">101</span>)
n =<span class="st"> </span><span class="dv">20</span>
X &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">vals =</span> <span class="kw">runif</span>(n), 
                <span class="dt">group =</span> <span class="kw">as.character</span>((<span class="dv">1</span>:n) %%<span class="st"> </span><span class="dv">2</span>))
<span class="kw">ggplot</span>(<span class="kw">head</span>(X, <span class="dv">6</span>), <span class="kw">aes</span>(<span class="dt">x =</span> vals)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">bw =</span> <span class="dv">1</span>, <span class="dt">position =</span> <span class="st">&quot;stack&quot;</span>, <span class="kw">aes</span>(<span class="dt">color =</span> group, <span class="dt">fill =</span> group)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="kw">aes</span>(<span class="dt">color =</span> group)) +<span class="st"> </span>
<span class="st"> </span><span class="co"># facet_grid( . ~ group) +</span>
<span class="st">  </span><span class="kw">xlim</span>(-<span class="fl">0.5</span>, <span class="fl">1.5</span>)</code></pre></div>
<p><img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p><a href="../Daily-Programming/Day-06-Programming-Task.pdf">Day 6 activity</a></p>
</div>
<div id="review-of-day-7" class="section level2">
<h2><span class="header-section-number">2.19</span> Review of Day 7</h2>
<ul>
<li>We finished reviewing adjusted R^2 and ANOVA.</li>
<li>Started talking about linear algebra.</li>
</ul>
<p>We only got through the first few elements in our review of linear algebra. Let’s go through them again</p>
</div>
<div id="using-predict-to-calculate-precision" class="section level2">
<h2><span class="header-section-number">2.20</span> Using predict() to calculate precision</h2>
<ul>
<li>confidence intervals</li>
<li>prediction intervals</li>
</ul>
</div>
<div id="conclusion" class="section level2">
<h2><span class="header-section-number">2.21</span> Conclusion</h2>
<p>This wraps up our look at linear regression. Main points:</p>
<ul>
<li>model output is a linear combination of the inputs.</li>
<li><code>lm()</code> finds the “best” linear combination.</li>
<li>rich theory relating to precision of coefficients and the residuals.</li>
<li>traditional ways of applying that theory: F tests and t tests.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="foundations-linear-algebra-likelihood-and-bayes-rule.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dtkaplan/math253/edit/master/210-Linear-Regression.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
