<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Notes for Math 253: Statistical Computing and Machine Learning</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Notes and other materials for Math 253 at Macalester College.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Notes for Math 253: Statistical Computing and Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and other materials for Math 253 at Macalester College." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Notes for Math 253: Statistical Computing and Machine Learning" />
  
  <meta name="twitter:description" content="Notes and other materials for Math 253 at Macalester College." />
  

<meta name="author" content="Daniel Kaplan">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduction.html">
<link rel="next" href="foundations-linear-algebra-likelihood-and-bayes-rule.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math 253 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#math-253-and-the-macalester-statistics-curriculum"><i class="fa fa-check"></i>Math 253 and the Macalester statistics curriculum</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#these-notes-are-written-in-bookdown"><i class="fa fa-check"></i>These notes are written in Bookdown</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#statistical-and-machine-learning"><i class="fa fa-check"></i><b>1.1</b> Statistical and Machine Learning</a><ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#example-1-machine-translation-of-natural-languages"><i class="fa fa-check"></i><b>1.1.1</b> Example 1: Machine translation of natural languages</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#example-2-from-library-catalogs-to-latent-semantic-indexing"><i class="fa fa-check"></i><b>1.1.2</b> Example 2: From library catalogs to latent semantic indexing</a></li>
<li class="chapter" data-level="1.1.3" data-path="introduction.html"><a href="introduction.html#computing-technique"><i class="fa fa-check"></i><b>1.1.3</b> Computing technique</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#review-of-day-1"><i class="fa fa-check"></i><b>1.2</b> Review of Day 1</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#theoretical-concepts-isl-2.1"><i class="fa fa-check"></i><b>1.3</b> Theoretical concepts ISL §2.1</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#statistics-concepts"><i class="fa fa-check"></i><b>1.3.1</b> Statistics concepts</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#computing-concepts"><i class="fa fa-check"></i><b>1.3.2</b> Computing concepts</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#cross-fertilization"><i class="fa fa-check"></i><b>1.3.3</b> Cross fertilization</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#many-techniques"><i class="fa fa-check"></i><b>1.4</b> Many techniques</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.4.1</b> Unsupervised learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction.html"><a href="introduction.html#supervised-learning"><i class="fa fa-check"></i><b>1.4.2</b> Supervised learning:</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#basic-dicotomies-in-machine-learning"><i class="fa fa-check"></i><b>1.5</b> Basic dicotomies in machine learning</a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#purposes-for-learning"><i class="fa fa-check"></i><b>1.5.1</b> Purposes for learning:</a></li>
<li class="chapter" data-level="1.5.2" data-path="introduction.html"><a href="introduction.html#dicotomies"><i class="fa fa-check"></i><b>1.5.2</b> Dicotomies</a></li>
<li class="chapter" data-level="1.5.3" data-path="introduction.html"><a href="introduction.html#prediction-versus-mechanism"><i class="fa fa-check"></i><b>1.5.3</b> Prediction versus mechanism</a></li>
<li class="chapter" data-level="1.5.4" data-path="introduction.html"><a href="introduction.html#flexibility-versus-variance"><i class="fa fa-check"></i><b>1.5.4</b> Flexibility versus variance</a></li>
<li class="chapter" data-level="1.5.5" data-path="introduction.html"><a href="introduction.html#black-box-vs-interpretable-models"><i class="fa fa-check"></i><b>1.5.5</b> Black box vs interpretable models</a></li>
<li class="chapter" data-level="1.5.6" data-path="introduction.html"><a href="introduction.html#reducible-versus-irreducible-error"><i class="fa fa-check"></i><b>1.5.6</b> Reducible versus irreducible error</a></li>
<li class="chapter" data-level="1.5.7" data-path="introduction.html"><a href="introduction.html#regression-versus-classification"><i class="fa fa-check"></i><b>1.5.7</b> Regression versus classification</a></li>
<li class="chapter" data-level="1.5.8" data-path="introduction.html"><a href="introduction.html#supervised-versus-unsupervised"><i class="fa fa-check"></i><b>1.5.8</b> Supervised versus unsupervised</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#programming-activity-1"><i class="fa fa-check"></i><b>1.6</b> Programming Activity 1</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#review-of-day-2"><i class="fa fa-check"></i><b>1.7</b> Review of Day 2</a><ul>
<li class="chapter" data-level="1.7.1" data-path="introduction.html"><a href="introduction.html#trade-offsdicotomies"><i class="fa fa-check"></i><b>1.7.1</b> Trade-offs/Dicotomies</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#a-classifier-example"><i class="fa fa-check"></i><b>1.8</b> A Classifier example</a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#programming-activity-2"><i class="fa fa-check"></i><b>1.9</b> Programming Activity 2</a></li>
<li class="chapter" data-level="1.10" data-path="introduction.html"><a href="introduction.html#day-3-theory-accuracy-precision-and-bias"><i class="fa fa-check"></i><b>1.10</b> Day 3 theory: accuracy, precision, and bias</a><ul>
<li class="chapter" data-level="1.10.1" data-path="introduction.html"><a href="introduction.html#figure-2.10"><i class="fa fa-check"></i><b>1.10.1</b> Figure 2.10</a></li>
<li class="chapter" data-level="1.10.2" data-path="introduction.html"><a href="introduction.html#another-example-a-smoother-simulated-fx."><i class="fa fa-check"></i><b>1.10.2</b> Another example: A smoother simulated <span class="math inline">\(f(x)\)</span>.</a></li>
<li class="chapter" data-level="1.10.3" data-path="introduction.html"><a href="introduction.html#whats-the-best-of-these-models"><i class="fa fa-check"></i><b>1.10.3</b> What’s the “best” of these models?</a></li>
<li class="chapter" data-level="1.10.4" data-path="introduction.html"><a href="introduction.html#why-is-testing-mse-u-shaped"><i class="fa fa-check"></i><b>1.10.4</b> Why is testing MSE U-shaped?</a></li>
<li class="chapter" data-level="1.10.5" data-path="introduction.html"><a href="introduction.html#measuring-the-variance-of-independent-sources-of-variation"><i class="fa fa-check"></i><b>1.10.5</b> Measuring the variance of independent sources of variation</a></li>
<li class="chapter" data-level="1.10.6" data-path="introduction.html"><a href="introduction.html#equation-2.7"><i class="fa fa-check"></i><b>1.10.6</b> Equation 2.7</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="introduction.html"><a href="introduction.html#programming-activity-3"><i class="fa fa-check"></i><b>1.11</b> Programming Activity 3</a></li>
<li class="chapter" data-level="1.12" data-path="introduction.html"><a href="introduction.html#review-of-day-3"><i class="fa fa-check"></i><b>1.12</b> Review of Day 3</a></li>
<li class="chapter" data-level="1.13" data-path="introduction.html"><a href="introduction.html#start-thursday-15-sept."><i class="fa fa-check"></i><b>1.13</b> Start Thursday 15 Sept.</a></li>
</ul></li>
<li class="part"><span><b>I Topic I: Linear Regression</b></span><ul>
<li class="chapter" data-level="1.14" data-path="introduction.html"><a href="introduction.html#day-4-preview"><i class="fa fa-check"></i><b>1.14</b> Day 4 Preview</a></li>
<li class="chapter" data-level="1.15" data-path="introduction.html"><a href="introduction.html#small-data"><i class="fa fa-check"></i><b>1.15</b> Small data</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>2</b> Notes</a></li>
<li class="chapter" data-level="3" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html"><i class="fa fa-check"></i><b>3</b> Foundations: linear algebra, likelihood and Bayes’ rule</a><ul>
<li class="chapter" data-level="3.1" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#linear-algebra"><i class="fa fa-check"></i><b>3.1</b> Linear Algebra</a></li>
<li class="chapter" data-level="3.2" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#arithmetic-of-linear-algebra-operations"><i class="fa fa-check"></i><b>3.2</b> Arithmetic of linear algebra operations</a></li>
<li class="chapter" data-level="3.3" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#the-geometry-of-fitting"><i class="fa fa-check"></i><b>3.3</b> The geometry of fitting</a></li>
<li class="chapter" data-level="3.4" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#precision-of-the-coefficients"><i class="fa fa-check"></i><b>3.4</b> Precision of the coefficients</a></li>
<li class="chapter" data-level="3.5" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#likelihood-and-bayes"><i class="fa fa-check"></i><b>3.5</b> Likelihood and Bayes</a></li>
<li class="chapter" data-level="3.6" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#summary-of-day-8"><i class="fa fa-check"></i><b>3.6</b> Summary of Day 8</a></li>
<li class="chapter" data-level="3.7" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#day-9-announcements"><i class="fa fa-check"></i><b>3.7</b> Day 9 Announcements</a><ul>
<li class="chapter" data-level="3.7.1" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#whats-a-probability"><i class="fa fa-check"></i><b>3.7.1</b> What’s a probability?</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#conditional-probability"><i class="fa fa-check"></i><b>3.8</b> Conditional probability</a></li>
<li class="chapter" data-level="3.9" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#inverting-conditional-probabilities"><i class="fa fa-check"></i><b>3.9</b> Inverting conditional probabilities</a></li>
<li class="chapter" data-level="3.10" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#summary-of-day-9"><i class="fa fa-check"></i><b>3.10</b> Summary of Day 9</a></li>
<li class="chapter" data-level="3.11" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#likelihood-example"><i class="fa fa-check"></i><b>3.11</b> Likelihood example</a></li>
<li class="chapter" data-level="3.12" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#exponential-probability-density"><i class="fa fa-check"></i><b>3.12</b> Exponential probability density</a><ul>
<li class="chapter" data-level="3.12.1" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#meanwhile-further-north"><i class="fa fa-check"></i><b>3.12.1</b> Meanwhile, further north …</a></li>
</ul></li>
<li class="chapter" data-level="3.13" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#california-earthquake-warning-reprise"><i class="fa fa-check"></i><b>3.13</b> California earthquake warning, reprise</a></li>
<li class="chapter" data-level="3.14" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#the-price-is-right"><i class="fa fa-check"></i><b>3.14</b> The Price is Right!</a></li>
<li class="chapter" data-level="3.15" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#from-likelihood-to-bayes"><i class="fa fa-check"></i><b>3.15</b> From likelihood to Bayes</a></li>
<li class="chapter" data-level="3.16" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#choosing-models-using-maximum-likelihood"><i class="fa fa-check"></i><b>3.16</b> Choosing models using maximum likelihood</a></li>
<li class="chapter" data-level="3.17" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#day-9-review"><i class="fa fa-check"></i><b>3.17</b> Day 9 Review</a></li>
<li class="chapter" data-level="3.18" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#reading-what-is-bayesian-statistics"><i class="fa fa-check"></i><b>3.18</b> Reading: <em>What is Bayesian Statistics</em></a></li>
<li class="chapter" data-level="3.19" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#programming-basics-conditionals"><i class="fa fa-check"></i><b>3.19</b> Programming Basics: Conditionals</a></li>
<li class="chapter" data-level="3.20" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#ifelse-examples"><i class="fa fa-check"></i><b>3.20</b> <code>ifelse()</code> examples</a></li>
<li class="chapter" data-level="3.21" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#if-else-examples"><i class="fa fa-check"></i><b>3.21</b> if … else … examples</a></li>
<li class="chapter" data-level="3.22" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#simple"><i class="fa fa-check"></i><b>3.22</b> Simple</a></li>
<li class="chapter" data-level="3.23" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#blood-testing"><i class="fa fa-check"></i><b>3.23</b> Blood testing</a></li>
<li class="chapter" data-level="3.24" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#the-hyper-volume-of-the-hypersphere."><i class="fa fa-check"></i><b>3.24</b> The (hyper)-volume of the hypersphere.</a></li>
<li class="chapter" data-level="3.25" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#find-the-surface-area-d_n-rn-1."><i class="fa fa-check"></i><b>3.25</b> Find the surface area, <span class="math inline">\(D_n r^{n-1}\)</span>.</a></li>
<li class="chapter" data-level="3.26" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html#in-class-programming-activity"><i class="fa fa-check"></i><b>3.26</b> In-class programming activity</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classifiers.html"><a href="classifiers.html"><i class="fa fa-check"></i><b>4</b> Classifiers</a><ul>
<li class="chapter" data-level="4.1" data-path="classifiers.html"><a href="classifiers.html#classification-overview"><i class="fa fa-check"></i><b>4.1</b> Classification overview</a></li>
<li class="chapter" data-level="4.2" data-path="classifiers.html"><a href="classifiers.html#day-10-preview"><i class="fa fa-check"></i><b>4.2</b> Day 10 preview</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5</b> Logistic Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#probability-and-odds"><i class="fa fa-check"></i><b>5.1</b> Probability and odds</a></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression.html"><a href="logistic-regression.html#log-odds"><i class="fa fa-check"></i><b>5.2</b> Log Odds</a></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression.html"><a href="logistic-regression.html#why-use-odds"><i class="fa fa-check"></i><b>5.3</b> Why use odds?</a></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression.html"><a href="logistic-regression.html#use-of-glm"><i class="fa fa-check"></i><b>5.4</b> Use of glm()</a></li>
<li class="chapter" data-level="5.5" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>5.5</b> Interpretation of coefficients</a></li>
<li class="chapter" data-level="5.6" data-path="logistic-regression.html"><a href="logistic-regression.html#example-logistic-regression-of-default"><i class="fa fa-check"></i><b>5.6</b> Example: Logistic regression of default</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html"><i class="fa fa-check"></i><b>6</b> Linear and Quadratic Discriminant Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#example-default-on-student-loans"><i class="fa fa-check"></i><b>6.1</b> Example: Default on student loans</a></li>
<li class="chapter" data-level="6.2" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#a-bayes-rule-approach"><i class="fa fa-check"></i><b>6.2</b> A Bayes’ Rule approach</a></li>
<li class="chapter" data-level="6.3" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#univariate-gaussian"><i class="fa fa-check"></i><b>6.3</b> Univariate Gaussian</a></li>
<li class="chapter" data-level="6.4" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#uncorrelated-bivariate-gaussian"><i class="fa fa-check"></i><b>6.4</b> Uncorrelated bivariate gaussian</a></li>
<li class="chapter" data-level="6.5" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#bivariate-normal-distribution-with-correlations"><i class="fa fa-check"></i><b>6.5</b> Bivariate normal distribution with correlations</a></li>
<li class="chapter" data-level="6.6" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#shape-of-multivariate-gaussian"><i class="fa fa-check"></i><b>6.6</b> Shape of multivariate gaussian</a></li>
<li class="chapter" data-level="6.7" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#generating-bivariate-normal-from-independent"><i class="fa fa-check"></i><b>6.7</b> Generating bivariate normal from independent</a></li>
<li class="chapter" data-level="6.8" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#independent-variables-x_i"><i class="fa fa-check"></i><b>6.8</b> Independent variables <span class="math inline">\(x_i\)</span></a></li>
<li class="chapter" data-level="6.9" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#re-explaining-boldsymbolsigma"><i class="fa fa-check"></i><b>6.9</b> Re-explaining <span class="math inline">\(\boldsymbol\Sigma\)</span></a></li>
<li class="chapter" data-level="6.10" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#lda"><i class="fa fa-check"></i><b>6.10</b> LDA</a></li>
<li class="chapter" data-level="6.11" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#qda"><i class="fa fa-check"></i><b>6.11</b> QDA</a></li>
<li class="chapter" data-level="6.12" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#error-test-rates-on-various-classifiers"><i class="fa fa-check"></i><b>6.12</b> Error test rates on various classifiers</a></li>
<li class="chapter" data-level="6.13" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#error-rates"><i class="fa fa-check"></i><b>6.13</b> Error rates</a></li>
<li class="chapter" data-level="6.14" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html#receiver-operating-curves"><i class="fa fa-check"></i><b>6.14</b> Receiver operating curves</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html"><i class="fa fa-check"></i><b>7</b> Cross-Validation and Bootstrapping</a><ul>
<li class="chapter" data-level="7.1" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#philosophical-approaches"><i class="fa fa-check"></i><b>7.1</b> Philosophical approaches</a><ul>
<li class="chapter" data-level="7.1.1" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#occams-razor-a-heuristic"><i class="fa fa-check"></i><b>7.1.1</b> Occam’s Razor: A heuristic</a></li>
<li class="chapter" data-level="7.1.2" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#einsteins-proverb"><i class="fa fa-check"></i><b>7.1.2</b> Einstein’s proverb</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#operationalizing-model-choice"><i class="fa fa-check"></i><b>7.2</b> Operationalizing model choice</a></li>
<li class="chapter" data-level="7.3" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#some-definitions-of-better"><i class="fa fa-check"></i><b>7.3</b> Some definitions of “better”</a></li>
<li class="chapter" data-level="7.4" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#training-and-testing"><i class="fa fa-check"></i><b>7.4</b> Training and Testing</a></li>
<li class="chapter" data-level="7.5" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#trade-off"><i class="fa fa-check"></i><b>7.5</b> Trade-off</a></li>
<li class="chapter" data-level="7.6" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#classical-theory-interlude"><i class="fa fa-check"></i><b>7.6</b> Classical theory interlude</a></li>
<li class="chapter" data-level="7.7" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html#bootstrapping"><i class="fa fa-check"></i><b>7.7</b> Bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html"><i class="fa fa-check"></i><b>8</b> Regularization, shrinkage and dimension reduction</a><ul>
<li class="chapter" data-level="8.1" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#best-subset-selection"><i class="fa fa-check"></i><b>8.1</b> Best subset selection</a></li>
<li class="chapter" data-level="8.2" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#approximation-to-best-subset-selection"><i class="fa fa-check"></i><b>8.2</b> Approximation to best subset selection</a></li>
<li class="chapter" data-level="8.3" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#classical-theory-of-best-model-choice"><i class="fa fa-check"></i><b>8.3</b> Classical theory of best model choice</a></li>
<li class="chapter" data-level="8.4" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#optimization"><i class="fa fa-check"></i><b>8.4</b> Optimization</a><ul>
<li class="chapter" data-level="8.4.1" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#what-are-we-optimizing-over"><i class="fa fa-check"></i><b>8.4.1</b> What are we optimizing over?</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#shrinkage-methods"><i class="fa fa-check"></i><b>8.5</b> Shrinkage methods</a><ul>
<li class="chapter" data-level="8.5.1" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#ridge-regression"><i class="fa fa-check"></i><b>8.5.1</b> Ridge regression</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#lasso"><i class="fa fa-check"></i><b>8.6</b> LASSO</a></li>
<li class="chapter" data-level="8.7" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#review"><i class="fa fa-check"></i><b>8.7</b> Review</a></li>
<li class="chapter" data-level="8.8" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#multi-collinearity"><i class="fa fa-check"></i><b>8.8</b> Multi-collinearity</a></li>
<li class="chapter" data-level="8.9" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#creating-correlations"><i class="fa fa-check"></i><b>8.9</b> Creating correlations</a></li>
<li class="chapter" data-level="8.10" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#rank-1-matrices"><i class="fa fa-check"></i><b>8.10</b> Rank 1 Matrices</a></li>
<li class="chapter" data-level="8.11" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#idea-of-singular-values."><i class="fa fa-check"></i><b>8.11</b> Idea of singular values.</a></li>
<li class="chapter" data-level="8.12" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html#dimension-reduction"><i class="fa fa-check"></i><b>8.12</b> Dimension reduction</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html"><i class="fa fa-check"></i><b>9</b> Nonlinearity in linear models</a><ul>
<li class="chapter" data-level="9.1" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#smoothers"><i class="fa fa-check"></i><b>9.1</b> Smoothers</a><ul>
<li class="chapter" data-level="9.1.1" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#ideas-of-smoothness"><i class="fa fa-check"></i><b>9.1.1</b> Ideas of smoothness</a></li>
<li class="chapter" data-level="9.1.2" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#polynomials"><i class="fa fa-check"></i><b>9.1.2</b> Polynomials</a></li>
<li class="chapter" data-level="9.1.3" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#the-model-matrix"><i class="fa fa-check"></i><b>9.1.3</b> The model matrix</a></li>
<li class="chapter" data-level="9.1.4" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#sigmoidal-functions"><i class="fa fa-check"></i><b>9.1.4</b> Sigmoidal Functions</a></li>
<li class="chapter" data-level="9.1.5" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#hat-functions"><i class="fa fa-check"></i><b>9.1.5</b> Hat functions</a></li>
<li class="chapter" data-level="9.1.6" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#fourier-analysis"><i class="fa fa-check"></i><b>9.1.6</b> Fourier analysis</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#steps"><i class="fa fa-check"></i><b>9.2</b> Steps</a></li>
<li class="chapter" data-level="9.3" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#other-functions"><i class="fa fa-check"></i><b>9.3</b> Other functions</a></li>
<li class="chapter" data-level="9.4" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#holes-in-the-data"><i class="fa fa-check"></i><b>9.4</b> Holes in the data</a></li>
<li class="chapter" data-level="9.5" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#bootstrapping-1"><i class="fa fa-check"></i><b>9.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="9.6" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#normal-theory-confidence-bands"><i class="fa fa-check"></i><b>9.6</b> Normal theory confidence bands</a></li>
<li class="chapter" data-level="9.7" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#splines"><i class="fa fa-check"></i><b>9.7</b> Splines</a><ul>
<li class="chapter" data-level="9.7.1" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#b-splines"><i class="fa fa-check"></i><b>9.7.1</b> B-splines</a></li>
<li class="chapter" data-level="9.7.2" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#natural-splines"><i class="fa fa-check"></i><b>9.7.2</b> Natural splines</a></li>
<li class="chapter" data-level="9.7.3" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#smoothing-splines"><i class="fa fa-check"></i><b>9.7.3</b> Smoothing splines</a></li>
<li class="chapter" data-level="9.7.4" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#smoothers-in-k-dimensions"><i class="fa fa-check"></i><b>9.7.4</b> Smoothers in k dimensions</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html#gams"><i class="fa fa-check"></i><b>9.8</b> GAMS</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="programming-activity.html"><a href="programming-activity.html"><i class="fa fa-check"></i><b>10</b> Programming Activity</a></li>
<li class="chapter" data-level="11" data-path="where-to-place-knots.html"><a href="where-to-place-knots.html"><i class="fa fa-check"></i><b>11</b> Where to place knots?</a></li>
<li class="chapter" data-level="12" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html"><i class="fa fa-check"></i><b>12</b> Trees for Regression and Classification</a><ul>
<li class="chapter" data-level="12.1" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html#splitting-criteria-for-classification-trees"><i class="fa fa-check"></i><b>12.1</b> Splitting Criteria for Classification Trees</a></li>
<li class="chapter" data-level="12.2" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html#variable-importance"><i class="fa fa-check"></i><b>12.2</b> Variable importance</a></li>
<li class="chapter" data-level="12.3" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html#avoiding-overfitting"><i class="fa fa-check"></i><b>12.3</b> Avoiding overfitting</a></li>
<li class="chapter" data-level="12.4" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html#pruning"><i class="fa fa-check"></i><b>12.4</b> Pruning</a></li>
<li class="chapter" data-level="12.5" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html#averaging"><i class="fa fa-check"></i><b>12.5</b> Averaging</a></li>
<li class="chapter" data-level="12.6" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html#shrinking-boosting"><i class="fa fa-check"></i><b>12.6</b> Shrinking (“Boosting”)</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html"><i class="fa fa-check"></i><b>13</b> Support Vector Classifiers</a><ul>
<li class="chapter" data-level="13.1" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#lines-planes-and-hyperplanes"><i class="fa fa-check"></i><b>13.1</b> Lines, planes, and hyperplanes</a><ul>
<li class="chapter" data-level="13.1.1" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#rescaling-x"><i class="fa fa-check"></i><b>13.1.1</b> Rescaling X</a></li>
<li class="chapter" data-level="13.1.2" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#impose-an-absolute-constraint"><i class="fa fa-check"></i><b>13.1.2</b> Impose an absolute constraint</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#optimizing-within-the-constraint"><i class="fa fa-check"></i><b>13.2</b> Optimizing within the constraint</a></li>
<li class="chapter" data-level="13.3" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#allowing-violations-of-the-boundary"><i class="fa fa-check"></i><b>13.3</b> Allowing violations of the boundary</a></li>
<li class="chapter" data-level="13.4" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#nonlinear-boundaries"><i class="fa fa-check"></i><b>13.4</b> Nonlinear Boundaries</a></li>
<li class="chapter" data-level="13.5" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#support-vector-machine"><i class="fa fa-check"></i><b>13.5</b> Support Vector Machine</a></li>
<li class="chapter" data-level="13.6" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#kernels"><i class="fa fa-check"></i><b>13.6</b> Kernels</a></li>
<li class="chapter" data-level="13.7" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html#svm-versus-logistic-regression"><i class="fa fa-check"></i><b>13.7</b> SVM versus logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="programming-basics.html"><a href="programming-basics.html"><i class="fa fa-check"></i><b>14</b> Programming Basics</a><ul>
<li class="chapter" data-level="14.1" data-path="programming-basics.html"><a href="programming-basics.html#programming-basics-i-names-classes-and-objects-progbasics1"><i class="fa fa-check"></i><b>14.1</b> Programming Basics I: Names, classes, and objects {progbasics1}</a><ul>
<li class="chapter" data-level="14.1.1" data-path="programming-basics.html"><a href="programming-basics.html#names"><i class="fa fa-check"></i><b>14.1.1</b> Names</a></li>
<li class="chapter" data-level="14.1.2" data-path="programming-basics.html"><a href="programming-basics.html#objects"><i class="fa fa-check"></i><b>14.1.2</b> Objects</a></li>
<li class="chapter" data-level="14.1.3" data-path="programming-basics.html"><a href="programming-basics.html#vectors"><i class="fa fa-check"></i><b>14.1.3</b> Vectors</a></li>
<li class="chapter" data-level="14.1.4" data-path="programming-basics.html"><a href="programming-basics.html#matrices"><i class="fa fa-check"></i><b>14.1.4</b> Matrices</a></li>
<li class="chapter" data-level="14.1.5" data-path="programming-basics.html"><a href="programming-basics.html#lists"><i class="fa fa-check"></i><b>14.1.5</b> Lists</a></li>
<li class="chapter" data-level="14.1.6" data-path="programming-basics.html"><a href="programming-basics.html#functions"><i class="fa fa-check"></i><b>14.1.6</b> Functions</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="programming-basics.html"><a href="programming-basics.html#programming-basics-linear-models"><i class="fa fa-check"></i><b>14.2</b> Programming basics: Linear Models</a><ul>
<li class="chapter" data-level="14.2.1" data-path="programming-basics.html"><a href="programming-basics.html#graphics-basics"><i class="fa fa-check"></i><b>14.2.1</b> Graphics basics</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="programming-basics.html"><a href="programming-basics.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>14.3</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="14.4" data-path="programming-basics.html"><a href="programming-basics.html#loopsiteration"><i class="fa fa-check"></i><b>14.4</b> Loops/Iteration</a></li>
<li class="chapter" data-level="14.5" data-path="programming-basics.html"><a href="programming-basics.html#parts-of-a-loop"><i class="fa fa-check"></i><b>14.5</b> Parts of a loop</a></li>
<li class="chapter" data-level="14.6" data-path="programming-basics.html"><a href="programming-basics.html#trivial-examples"><i class="fa fa-check"></i><b>14.6</b> Trivial examples</a></li>
<li class="chapter" data-level="14.7" data-path="programming-basics.html"><a href="programming-basics.html#bootstrapping-2"><i class="fa fa-check"></i><b>14.7</b> Bootstrapping</a></li>
<li class="chapter" data-level="14.8" data-path="programming-basics.html"><a href="programming-basics.html#leave-one-out-cross-validation."><i class="fa fa-check"></i><b>14.8</b> Leave-one-out cross-validation.</a></li>
<li class="chapter" data-level="14.9" data-path="programming-basics.html"><a href="programming-basics.html#building-a-package"><i class="fa fa-check"></i><b>14.9</b> Building a package</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i>Appendices</a></li>
<li class="chapter" data-level="" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html"><i class="fa fa-check"></i>Connecting RStudio to your GitHub repository</a><ul>
<li class="chapter" data-level="14.10" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html#setting-up-rstudio"><i class="fa fa-check"></i><b>14.10</b> Setting up RStudio</a></li>
<li class="chapter" data-level="14.11" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html#setting-up-your-math-253-repository"><i class="fa fa-check"></i><b>14.11</b> Setting up your Math 253 repository</a></li>
<li class="chapter" data-level="14.12" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html#using-your-repository"><i class="fa fa-check"></i><b>14.12</b> Using your repository</a></li>
<li class="chapter" data-level="14.13" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html#why-are-we-doing-this"><i class="fa fa-check"></i><b>14.13</b> Why are we doing this?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="instructions-for-the-publishing-system-bookdown.html"><a href="instructions-for-the-publishing-system-bookdown.html"><i class="fa fa-check"></i>Instructions for the publishing system: Bookdown</a></li>
<li class="divider"></li>
<li><a href="https://github.com/dtkaplan/math253" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for Math 253: Statistical Computing and Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="notes" class="section level1">
<h1><span class="header-section-number">Topic 2</span> Notes</h1>
<!-- 

## Review of Day 4, Sept 15, 2016

* How the linear regression architecture relates to machine learning.
    - linear regression is designed to work even in "small data" situations where
        a. cross-validation is not appropriate
        b. the number of model degrees of freedom may be almost as large as $n$
    - provides a ready definition of the "size" of a model: the number of coefficients.
* Main software used in linear regression, `lm()` and `predict()`
* Programming basics: indexing of vectors, matrices and data frames.



## Regression and Interpretability

Regression models are generally constructed for the sake of interpretability:

* Global linearity
* Coefficients are indication of effect size. The coefficients have physical units.
* Term by term indication of statistical significance

An example on `College` data from `ISLR` package 


```r
data(College, package="ISLR")
College$Yield <- with(College, Enroll/Accept)
mod1 <- lm(Yield ~ Outstate + Grad.Rate + Top25perc, data = College)
mosaic::rsquared(mod1)
```

```
## [1] 0.2170221
```

```r
mod2 <- lm(Yield ~ . - Grad.Rate, data = College)
mosaic::rsquared(mod2)
```

```
## [1] 0.5004599
```
* What variables matter?
* How good are the predictions?
* How strong are the effects?

## Toward an automated regression process

In machine learning, we ask the computer to identify patterns in the data. 

* In "traditional" regression (which is still very important), we specify the explanatory terms and the computer finds the "best" model with those terms: least squares. 
* In machine learning, we want the computer to figure out which terms, of all the possibilities, will lead to the "best" model.


## Selecting model terms

The regression techniques 

- Traditional regression: 
    - Our knowledge of the system being studied.
    - Heirarchical principal
        * main effects, then
        * interaction.
- Machine learning
    - Look at all combinations of variables? 
    - Activity 1: 
        * Write a statement that will pull 2 random variables from a data frame *and* the explanatory variable.
        * Use `Yield ~ .` as the formula.

```r
explanatory_vars <- names(College)[-19]

my_vars <- sample(explanatory_vars, size = 12)
new_data <- College[ , c(my_vars, "Yield")]
mod <- lm(Yield ~ ., data = new_data)
mosaic::rsquared(mod)
```

```
## [1] 0.4825071
```

```r
mod
```

```
## 
## Call:
## lm(formula = Yield ~ ., data = new_data)
## 
## Coefficients:
## (Intercept)     Outstate        Books    Top10perc    S.F.Ratio  
##   5.337e-01   -1.138e-05    2.493e-05    9.238e-04    1.919e-03  
##        Apps     Personal          PhD       Accept    Top25perc  
##   1.119e-05    2.283e-06   -9.305e-04   -8.383e-05   -4.290e-04  
##      Enroll   PrivateYes       Expend  
##   1.721e-04    1.027e-02    1.381e-06
```

```r
2^18
```

```
## [1] 262144
```
        
- Activity 2: 
        * How many combinations are there of $k$ explanatory variables? Calculate this for $k = 5, 10, 15, 20$. How many are there in the `College`?
        * What about with interactions?
- How long does it take to fit a model?

```r
system.time( do(1000) * lm(Yield ~ ., data=College))
```

```
##    user  system elapsed 
##   6.224   0.072   6.376
```

```r
256*7/3600
```

```
## [1] 0.4977778
```


```r
names(College)
```

```
##  [1] "Private"     "Apps"        "Accept"      "Enroll"      "Top10perc"  
##  [6] "Top25perc"   "F.Undergrad" "P.Undergrad" "Outstate"    "Room.Board" 
## [11] "Books"       "Personal"    "PhD"         "Terminal"    "S.F.Ratio"  
## [16] "perc.alumni" "Expend"      "Grad.Rate"   "Yield"
```
    
With $k$ explanatory variables, $2^k$ possibilities, not even including interactions. Including first-order interactions, it's
    $2^k + 2^{k(k-1)/2}$. Calculate this for $k=3.
    - Increase in $R^2$? Problem: $R^2$ will always go up as we add a new term.
    - Some other measure that takes into account how much $R^2$ should go up.



## Programming basics: Graphics


```r
plot(1, type = "n", xlim = c(100,200), ylim = c(300,500))
```

<img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-9-1.png" width="672" />

Basic functions:

1. Create a frame: `plot()`.  Blank frame: `plot( , type="n")`
    - set axis limits, 
2. Dots: `points(x, y)`, `pch=20`
3. Lines: `lines(x, y)` --- with `NA` for line breaks
4. Polygons: `polygon(x, y)` --- like lines but connects first to last.
    - fill
5. Color, size, ... `rgb(r, g, b, alpha)`, "tomato"



## In-class programming activity 

[Programming Activity 4](../Daily-Programming/Day-4-Programming-Task.pdf): Drawing a histogram.

## Day 5 Summary

### Linear regression

* Discussed "interpretability" of linear models, e.g. meaning of coefficients, confidence intervals, R^2, etc.
    - which variables are "important" via ANOVA and mean sum of squares
* Discussed metrics to compare models 
    - R^2 -- not fair, since "bigger" models are always better
    - Punishment: Two criteria for judging
        - R^2
        - How big the model is.
        - These two are somehow combined together into "adjusted R^2." We'll say more about that today.
    - Cross-validation. Judge each model on its "out of sample" prediction performance.

### Coefficients as quantities

Coefficients in linear models are not just numbers, they are physical quantities with dimensions and units.

* Dimensions are always (dim of response)/(dim of this term)
* The model doesn't depend on the units of these quantities.  The units only set the magnitude to the numerical part of the coefficient, but as a quantity a coefficient is the same thing regardless of units.
* Conversion from one unit to another by multiplying by 1, but expressed in different units, e.g. 60 seconds per minute, 2.2 pounds per kilogram.





## In-class programming activity 

[Day 5 activity](../Daily-Programming/Day-05-Programming-Task.pdf)

Drawing a histogram.

## Day 6 Summary

* $R^2$ 
    - var(fitted) / var(response)
    - partitioning of variance: 
        - var(fitted) + var(resid) = var(response)
        - same with sum of squares: SS(fitted) + SS(resid) = SS(response)
* Adjusted $R^2$
    - $R^2$ vs $p$ picture
    - Derive a formula from the picture: we've got $p+1$ df to get from $R^2 = 0$ to our observed $R^2$, so $n - (p+1)$ df left for the residuals. Rate of increase due to junk is $(1 - R^2) / (n - p - 1)$. Projecting back $p$ terms gives an adjustment of $(1 - R^2) / \frac{p}{n - p - 1}$. Subtract this from $R^2$.
    - Wikipedia gives two formulas:
        - $Adj R^2 = {1-(1-R^{2}){n-1 \over n-p-1}}$ -- this projects back $n-1$ terms from 1.
        - $Adj R^2 = {R^{2}-(1-R^{2}){p \over n-p-1}}$ --- projects back $p$ terms from $R^2$.
        
- Adjusted $R^2$
- Whole model ANOVA.
- ANOVA on model parts


## Measuring Accuracy of the Model

* $R^2$ = Var(fitted)/Var(response)
* Adjusted $R^2$ - takes into account estimate of average increase in $R^2$ per junk degree of freedom
* Residual Standard Error - Sqrt of average square error per residual degree of freedom. The sqrt of the mean square for residuals in ANOVA.

## Bias of the model

You need to know the "truth" to calculate the bias. We don't.

<img src="Images/Chapter-2/2.1.png" width="400" />

* Perhaps effect of TV goes as sqrt(money) as media get saturated?
* Perhaps there is a synergy that wasn't included in the model?

### Theory of whole-model ANOVA.

Standard measure: 
$\frac{\mbox{Explained amount}}{\mbox{Unexplained amount}}$

Examples:

- Standard error of mean:  $\frac{\hat{\mu}}{\sigma / n}$ -- note the $n$.
- t statistic on difference between two means: $\frac{\hat{\mu}_1 - \hat{\mu}_2}{\sigma / (n-1)}$
- F statistic: $\frac{SS / df1}{SSR / df2}$
    - df1 is the number of degrees of freedom involved by the model or model term under consideration.
    - df2 is $n - (p - 1)$ where $p$ is the total degrees of freedom in the model.  (I called this $m$ in the Math 155 book.)  The intercept is what the $-1$ is about: the intercept *can never* account for case-to-case variation.

Trade-off between eating variance and consuming degrees of freedom.




## Forward, backward and mixed selection

Use the `College` model to demonstrate each of the approaches by hand.  Start with `pairs()` or write an `lapply()` for the correlation with `Yield`?

Create a whole bunch of model terms

- "main" effects
- "interaction" effects
- nonlinear transformations: powers, logs, sqrt, steps, ...
- categorical variables

Result: a set of $k$ vectors that we're interested to use in our model.

Considerations:

- not all of the $k$ vectors may pull their weight
- two or more vectors may overlap in how they eat up variance

Algorithmic approaches:

- Try all combinations, pick the best one.
    - computationally expensive/impossible $2^k$ possibilities
    - what's the sensitivity of the process to the choice of training data?
- "Greedy" approaches


     
## Programming Basics: Functions

1. Syntax of functions:
```r
name <- function(arg1, arg2, ...) {
  body of function. Can use arg1, arg2, etc. 
}
```

* typically you will return a value. The value calculated by the last command line in the body is what's returned. Or you can use `return()` at any point in the function.
* Often functions are designed to produce "side effects", e.g. graphics.  
* Scope: what happens in functions stays in functions.

1. Create a plotting frame: `plot()`
    - Write a function that makes this more convenient to use. What features would you like. 
    
    ```r
    blank_frame <- function(xlim, ylim) {
      
    }
    ```
2. Write a function to draw a circle.
    - What do you want the interface to look like? What arguments are essential? What options are nice to have?
    


## In-class programming activity

Histogram and density functions 



```r
set.seed(101)
n = 20
X <- data.frame(vals = runif(n), 
                group = as.character((1:n) %% 2))
ggplot(head(X, 6), aes(x = vals)) + 
  geom_density(bw = 1, position = "stack", aes(color = group, fill = group)) + 
  geom_rug(aes(color = group)) + 
 # facet_grid( . ~ group) +
  xlim(-0.5, 1.5)
```

<img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-12-1.png" width="672" />

[Day 6 activity](../Daily-Programming/Day-06-Programming-Task.pdf)

 
     
## Review of Day 7

- We finished reviewing adjusted R^2 and ANOVA.
- Started talking about linear algebra.

We only got through the first few elements in our review of linear algebra.  Let's go through them again




## Using predict() to calculate precision

- confidence intervals
- prediction intervals

## Conclusion

This wraps up our look at linear regression. Main points:

* model output is a linear combination of the inputs.
* `lm()` finds the "best" linear combination.
* rich theory relating to precision of coefficients and the residuals.
* traditional ways of applying that theory: F tests and t tests.




</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="foundations-linear-algebra-likelihood-and-bayes-rule.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dtkaplan/math253/edit/master/210-Linear-Regression.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
